{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new GAOGB.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fAg1n6g1vHiD",
        "J5OPtKScxSLR",
        "IBV922GkxEhk",
        "dbTQAhftxlDN",
        "jAO5Y00vyD66",
        "voJCu7emyQ1Y",
        "N2c5i5HoyarB",
        "0M-Svs5Ly4vu",
        "YCUS5Z21zR7W",
        "YKtfvHR5zdU8",
        "oLTbB-Gazs7i",
        "Jg5HiAkfFeU7",
        "KC2gOaGIvz0u",
        "ExzYZxUy0Jae",
        "yjmjSbwUz6Zi",
        "1uS3T9LO0QBf",
        "emKTmFqy0aHH",
        "oP9qWNMyiTko",
        "E2scSK4c68af",
        "xctIhfkxOM2M",
        "IsoO4TAjOUSP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameneh1376/winter_AI_school/blob/master/new_GAOGB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JZF3UYfumct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "ef466c03-e3c9-48da-c6d8-d2c83d33b637"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAg1n6g1vHiD"
      },
      "source": [
        "# **load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj3l4zzzux1z"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "def load_data(dataset,subject_number):\n",
        "\n",
        "    if dataset ==\"eeg\":\n",
        "        subject_train=np.load(\"/content/drive/My Drive/data/train_subject.npy\")\n",
        "        subject_test=np.load(\"/content/drive/My Drive/data/test_subject.npy\")\n",
        "        X_tr=np.load(\"/content/drive/My Drive/data/train_latent.npy\")\n",
        "        X_te=np.load(\"/content/drive/My Drive/data/test_latent.npy\")\n",
        "        y_tr=np.load(\"/content/drive/My Drive/data/train_label.npy\")\n",
        "        y_te=np.load(\"/content/drive/My Drive/data/test_label.npy\")\n",
        "        X=np.array((8357,64))\n",
        "        y=np.array((8357,1))\n",
        "        \n",
        "\n",
        "        X=np.vstack((X_te,X_tr))\n",
        "        y=np.vstack((y_te,y_tr))\n",
        "        # for i in range(y.shape[0]):\n",
        "        #   if y[i][0]==0:\n",
        "        #     y[i][0]=-1.0\n",
        "\n",
        "\n",
        "        subject=np.vstack((subject_test,subject_train))\n",
        "        # kf = KFold(n_splits=3) # Define the split - into 2 folds \n",
        "        # kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
        "        # # print(kf) \n",
        "        \n",
        "        # for train_index, test_index in kf.split(X):\n",
        "        #   # print \"TRAIN:\", train_index, \"TEST:\", test_index\n",
        "        #   X_train, X_test = X[train_index], X[test_index]\n",
        "        #   y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        X_test= np.zeros((1,64))\n",
        "        X_train=np.zeros((1,64))\n",
        "        y_test=np.zeros((1,1))\n",
        "        y_train=np.zeros((1,1))\n",
        "        \n",
        "        for i in range(X.shape[0]):\n",
        "          if subject[i]==subject_number:\n",
        "            if i==0:\n",
        "              X_test=X[i]\n",
        "              y_test=y[i]\n",
        "            else:  \n",
        "              X_test=np.vstack((X_test,X[i]))\n",
        "              y_test=np.vstack((y_test,y[i]))\n",
        "              \n",
        "          else:\n",
        "            if i==0:\n",
        "              X_train=X[i]\n",
        "              y_train=y[i]\n",
        "            else:  \n",
        "              X_train=np.vstack((X_train,X[i]))\n",
        "              y_train=np.vstack((y_train,y[i]))\n",
        "        sd=np.max(X_train)-np.min(X_train)\n",
        "        minn=np.min(X_train)\n",
        "        X_train=(X_train-minn)/(sd)\n",
        "        X_test=(X_test-minn)/(sd)\n",
        "        # X_new, X_train_genetic, y_new, y_train_genetic = train_test_split(X,y,test_size = 0.08, random_state = 1)\n",
        "        # X_train, X_test, y_train, y_test = train_test_split(X_new,y_new,test_size = 0.20, random_state = 1)\n",
        "    elif dataset ==\"p300\":  \n",
        "\n",
        "        subject_train=np.load(\"/content/drive/My Drive/train_subject.npy\")\n",
        "        subject_test=np.load(\"/content/drive/My Drive/test_subject.npy\")\n",
        "        # X=np.load(\"/content/drive/My Drive/data/p300.npy\")\n",
        "        y_tr=np.load(\"/content/drive/My Drive/data/train_label.npy\")\n",
        "        y_te=np.load(\"/content/drive/My Drive/data/test_label.npy\")\n",
        "        # X=np.array((8357,64))\n",
        "        y=np.array((8357,1))\n",
        "        \n",
        "        # X=np.vstack((X_te,X_tr))\n",
        "        y=np.vstack((y_te,y_tr))\n",
        "        subject=np.vstack((subject_test,subject_train))\n",
        "\n",
        "    #     X_test= np.zeros((1,31))\n",
        "    #     X_train=np.zeros((1,31))\n",
        "    #     y_test=np.zeros((1,1))\n",
        "    #     y_train=np.zeros((1,1))  \n",
        "    #     for i in range(p300.shape[0]):\n",
        "    #       if subject[i]==subject_number:\n",
        "    #         if X_test.shape[0]==1:\n",
        "    #           X_test=p300[i]\n",
        "    #           y_test=y[i]\n",
        "    #         else:  \n",
        "    #           X_test=np.vstack((X_test,p300[i]))\n",
        "    #           y_test=np.vstack((y_test,y[i]))\n",
        "              \n",
        "    #       else:\n",
        "    #         if X_train.shape[0]==1:\n",
        "    #           X_train=p300[i]\n",
        "    #           y_train=y[i]\n",
        "    #         else:  \n",
        "    #           X_train=np.vstack((X_train,p300[i]))\n",
        "    #           y_train=np.vstack((y_train,y[i]))\n",
        "\n",
        "    # elif dataset ==\"cancer\":\n",
        "    #     data = datasets.load_breast_cancer()\n",
        "    #     X = data.data\n",
        "    #     y = data.target\n",
        "    #     y_clean = []\n",
        "    #     for item in y:\n",
        "    #         if item==1:\n",
        "    #             y_clean.append(item)\n",
        "    #         elif item==0:\n",
        "    #             y_clean.append(-1)\n",
        "    #     y = np.array(y_clean)\n",
        "    #     # X_new, X_adaptive, y_new, y_adaptive = train_test_split(X,y,test_size = 0.05, random_state = 1)\n",
        "        \n",
        "    #     X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state = 1)\n",
        "\n",
        "        \n",
        "    return X_train,X_test, y_train,y_test,X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGOX7O5xxzlL"
      },
      "source": [
        "# **ensembles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5OPtKScxSLR"
      },
      "source": [
        "## osb **boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjEmzEElxVcG"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class OSBoost(object):\n",
        "\n",
        "\tdef __init__(self, learners, classes, total_points = 10, gamma = 0.1):\n",
        "\t\tself.M = total_points\n",
        "\t\tself.learners = [learners(classes) for i in range(self.M)]\n",
        "\t\tself.alpha = np.ones(self.M)/self.M\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.theta = self.gamma/(2 + self.gamma)\n",
        "\n",
        "\n",
        "\tdef update(self, X, y):\n",
        "\t\tz_t = 0.0\n",
        "\t\tw = 1.0\n",
        "\t\tfor learner in self.learners:\n",
        "\t\t\tz_t += learner.predict(X)*y - self.theta\n",
        "\t\t\tlearner.partial_fit(X, y, sample_weight = w)\n",
        "\t\t\tif z_t <=0:\n",
        "\t\t\t\tw = 1.0\n",
        "\t\t\telse:\n",
        "\t\t\t\tw = (1.0 - self.gamma)** (z_t/2.0)\n",
        "\n",
        "\tdef raw_predict(self, X):\n",
        "\t\treturn sum(learner.predict(X) for learner in self.learners)\n",
        "\n",
        "\tdef classify(self, X):\n",
        "\t\tlabel_weights = {}\n",
        "\t\tfor i in range(self.M):\n",
        "\t\t\tpred_y = self.learners[i].predict(X)\n",
        "\t\t\tif pred_y in label_weights.iterkeys():\n",
        "\t\t\t\tlabel_weights[pred_y] += self.alpha[i]\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabel_weights[pred_y] = self.alpha[i]\n",
        "\n",
        "\t\tkey = max(label_weights.iterkeys(), key=(lambda key: label_weights[key]))\n",
        "\t\treturn key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBV922GkxEhk"
      },
      "source": [
        "## exp **boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGvLj70xGgC"
      },
      "source": [
        "\"\"\"\n",
        "    An online boosting algorithm which mixes SmoothBoost with\n",
        "    \"Learning from Expert Advice\" from Chen '12.\n",
        "\"\"\"\n",
        "\n",
        "from random import random\n",
        "from collections import defaultdict\n",
        "# from osboost import OSBoost\n",
        "\n",
        "\n",
        "def choose(p):\n",
        "    r = random()\n",
        "    n = len(p)\n",
        "    p /= sum(p)\n",
        "    cdf = 0.0\n",
        "    for i in range(n):\n",
        "        cdf += p[i]\n",
        "        if r < cdf:\n",
        "            return i + 1\n",
        "    return n\n",
        "\n",
        "\n",
        "class EXPBoost(OSBoost):\n",
        "\n",
        "    def update(self, features, label):\n",
        "        beta = 0.5\n",
        "        exp_predict = 0.0\n",
        "\n",
        "        for i, learner in enumerate(self.learners):\n",
        "            exp_predict += learner.predict(features)\n",
        "            if exp_predict * label <= 0:\n",
        "                self.alpha[i] *= beta\n",
        "\n",
        "        super(EXPBoost, self).update(features, label)\n",
        "\n",
        "    def classify(self, features):\n",
        "        k = choose(self.alpha)\n",
        "        label_weights = defaultdict(int)\n",
        "        for i in range(k):\n",
        "            label = self.learners[i].predict(features)\n",
        "            label_weights[label] += self.alpha[i]\n",
        "\n",
        "        return max(label_weights.iterkeys(), key=(lambda key: label_weights[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbTQAhftxlDN"
      },
      "source": [
        "## oza **boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSIIFmbOxkYE"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        ":mod:`Ozaboost`\n",
        "==================\n",
        ".. module:: Ozaboost\n",
        "   :platform: Mac OS X\n",
        "   :synopsis:\n",
        ".. moduleauthor:: deshana.desai@nyu.edu\n",
        "Created on 2017-11-19, 5:00\n",
        "\"\"\"\n",
        "from random import shuffle\n",
        "from collections import defaultdict\n",
        "from math import log\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import poisson, seed\n",
        "\n",
        "class OzaBoostClassifier():\n",
        "\tdef __init__(self, learners, classes, total_points):\n",
        "\t\tself.total_points = total_points\n",
        "\t\tself.classes = classes\n",
        "\t\tself.learners = [learners(classes) for i in range(self.total_points)]\n",
        "\t\t#self.learners = [DecisionTree(classes) for i in range(self.total_points)]\n",
        "\t\t#self.learners = [perceptron.Perceptron(classes) for i in range(self.total_points)]\n",
        "\t\t#self.learners = [sgd_classifier(classes) for i in range(self.total_points)]\n",
        "\t\tself.correct = [0.0 for i in range(self.total_points)]\n",
        "\t\tself.incorrect = [0.0 for i in range(self.total_points)]\n",
        "\t\tself.error = [0.0 for i in range(self.total_points)]\n",
        "\t\tself.coeff = [0.0 for i in range(self.total_points)]\n",
        "\n",
        "        def get_error_rate(self, predictions, Y):\n",
        "\t\t\ttemp = zip(predictions, Y)\n",
        "\t\t\tcorrect = 0.0\n",
        "\t\t\tfor (p,y) in temp:\n",
        "\t\t\t\tif p==y:\n",
        "\t\t\t\t\tcorrect+=1\n",
        "\t\t\treturn float(correct)/float(len(Y))\n",
        "\n",
        "\tdef pretrain(self, train_X, train_Y, X_val, y_val):\n",
        "\t\terrors = []\n",
        "\t\timport random\n",
        "\t\tdata = zip(train_X, train_Y)\n",
        "\t\tfor i, learner in enumerate(self.learners):\n",
        "\t\t\tdatapoints_x = []\n",
        "\t\t\tdatapoints_y = []\n",
        "\t\t\twhile True:\n",
        "\t\t\t\tindex_point = random.randint(0,train_X.shape[0]-1)\n",
        "\t\t\t\tdatapoints_x.append(train_X[index_point])\n",
        "\t\t\t\tdatapoints_y.append(train_Y[index_point])\n",
        "\t\t\t\tlearner.fit(datapoints_x, datapoints_y, self.classes)\n",
        "\t\t\t\ttest_error = self.get_error_rate(learner.model.predict(X_val), y_val)\n",
        "\t\t\t\tif test_error>0.5:\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\t#train_X, train_Y, test_X, test_Y = train_test_split(data, test_size = 0.2)\n",
        "\t\t\t#learner.model.fit(datapoint_x, train_Y)\n",
        "\t\t\t#learner.model.fit(datapoints_x,datapoints_y)\n",
        "\t\t\t#test_error = learner.model.predict(X_val)\n",
        "\t\t\ttraining_error = learner.model.predict(train_X)\n",
        "\t\t\terrors.append((self.get_error_rate(training_error, train_Y),test_error))\n",
        "\t\treturn errors\n",
        "\n",
        "\tdef update(self, X, Y):\n",
        "\t\tweight = 1.0\n",
        "\t\t#shuffle(self.learners)\n",
        "\t\tfor i, learner in enumerate(self.learners):\n",
        "\t\t\t#print \"Round: \",i\n",
        "\t\t\tk = poisson(weight)\n",
        "\t\t\t#print \"Poisson Dist: \",k\n",
        "\t\t\tif k<=0:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tfor j in range(k):\n",
        "\t\t\t\tlearner.partial_fit(X,Y)\n",
        "\t\t\t\n",
        "\t\t\tprediction = learner.predict(X)\n",
        "\t\t\t\n",
        "\t\t\t#tree.export_graphviz(learner.model, out_file = \"learner_\"+str(i)+\".dot\")\n",
        "\t\t\t#print \"Initial Weight: \", weight\n",
        "\n",
        "\t\t\tif prediction == Y:\n",
        "\t\t\t\tself.correct[i] = self.correct[i]+weight\n",
        "\t\t\t\tN = float(self.correct[i]+self.incorrect[i])\n",
        "\t\t\t\ttemp = (N/(2.0*float(self.correct[i])))\n",
        "\t\t\t\t\n",
        "\t\t\t\tweight *= temp\n",
        "\t\t\t\t#print \"Weight of example has decreased to: \",weight\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.incorrect[i] = self.incorrect[i]+weight\n",
        "\t\t\t\tN = float(self.correct[i]+self.incorrect[i])\n",
        "\t\t\t\tweight = weight*(N/(2*self.incorrect[i]))\n",
        "\t\t\t\t#print \"Weight of example has increased to: \",weight\n",
        "\n",
        "\tdef classify(self,X):\t\n",
        "\t\tlabel_weights = {}\n",
        "\t\t\n",
        "\t\tfor i, learner in enumerate(self.learners):\n",
        "\t\t\tN = self.correct[i]+self.incorrect[i]+ 1e-16\n",
        "\t\t\tself.error[i] = (self.incorrect[i]+ 1e-16)/N\n",
        "\t\t\tself.coeff[i] = (self.error[i]+ 1e-16)/(1.0-self.error[i]+ 1e-16)\n",
        "\n",
        "\t\tfor i,learner in enumerate(self.learners):\n",
        "\t\t\tweight_learner = log(1/self.coeff[i])\n",
        "\t\t\tlabel = learner.predict(X)\n",
        "\t\t\tif label in label_weights.keys():\n",
        "\t\t\t\tlabel_weights[label] += weight_learner\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabel_weights[label] = weight_learner\n",
        "\t\t#print(\"learner \",i,\": \",format(weight_learner,\"0.2f\"))\n",
        "\n",
        "\t\treturn max(label_weights.iterkeys(), key = (lambda key: label_weights[key]))\n",
        "\t\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAO5Y00vyD66"
      },
      "source": [
        "## **smooth boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-vYGf4cyHU1"
      },
      "source": [
        "  \n",
        "from math import e\n",
        "\n",
        "class SmoothBoost(object):\n",
        "\t# Learner = weak_learner, classes = self.classes, M= m,trials=1,alpha=alpha,ta=ta,gama=gama\n",
        "\tdef __init__(self, Learner, classes, M,trials,alpha,ta,gama):\n",
        "\t\tself.classes = classes\n",
        "\t\tself.M = M\n",
        "\t\tself.learners = [Learner(classes,alpha,ta) for i in range(self.M)]\n",
        "\n",
        "\tdef update(self, X, y):\n",
        "\t\tf = 0\n",
        "\t\tw = 0.5\n",
        "\n",
        "\t\tfor learner in self.learners:\n",
        "\t\t\tlearner.partial_fit(X, y, sample_weight = w)\n",
        "\t\t\tf += learner.predict(X)\n",
        "\t\t\tw = 1.0 / (1.0 + e**(y*f))\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\tlabel_weights = {}\n",
        "\t\tfor i, learner in enumerate(self.learners):\n",
        "\t\t\tpred_y = learner.predict(X)\n",
        "\t\t\tif pred_y in label_weights.keys():\n",
        "\t\t\t\tlabel_weights[pred_y] += 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabel_weights[pred_y] = 1\n",
        "\t        return max(label_weights.iterkeys(), key=(lambda key: label_weights[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voJCu7emyQ1Y"
      },
      "source": [
        "## **GAGOB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPUVZiIdyUNr"
      },
      "source": [
        "\"\"\"\n",
        "    An Online Gradient Boost implementaton from Leistner.\n",
        "\"\"\"\n",
        "\n",
        "from math import log, e ,exp\n",
        "from sys import maxint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class GAOGBooster(object):\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(x):\n",
        "        return log(1.0 + e ** (-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def dloss(x):\n",
        "        return - 1.0 / (1.0 + e ** x)\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, Learner, classes, M , trials ,alpha,ta,gama):\n",
        "        self.wts=0\n",
        "        self.t=0\n",
        "        self.M = M\n",
        "        self.K = trials\n",
        "        self.learners = [[Learner(classes,alpha,ta) for _ in range(self.K)]\n",
        "                         for _ in range(self.M)]\n",
        "        self.errors = np.zeros((self.M, self.K))\n",
        "        self.w = []\n",
        "        self.f = [learners[0] for learners in self.learners]\n",
        "        # self.regression = LinearRegression() \n",
        "        self.sigma = np.zeros( self.M )\n",
        "        self.gama=gama\n",
        "        self.ta=ta\n",
        "        self.B = min( self.gama * self.M , log( 4.0 / self.gama ) )\n",
        "        self.LB = exp( self.B ) / ( 1 + exp( self.B ) )\n",
        "        w=np.zeros((1))\n",
        "    def update(self, features, label ):\n",
        "\n",
        "        self.t+=1\n",
        "        w = -GAOGBooster.dloss(0)\n",
        "        F = np.zeros(1+self.M)\n",
        "        \n",
        "        for m in range(self.M):\n",
        "            \n",
        "            # Track best of the K learners for this selector\n",
        "            best_k = 0\n",
        "            # min_error = maxint\n",
        "            # print \"m=\",m\n",
        "            # for k in range(self.K):\n",
        "            \n",
        "            h = self.learners[m][0]\n",
        "            # if self.t > 2:\n",
        "            #   # if m > 0:\n",
        "            #   #   h1=self.learners[m-1][0]\n",
        "            #   if h.predict(features) != label or label[0]==1:\n",
        "            #     # self.errors[m][0]+=1\n",
        "            #     h.partial_fit(features,w,W) \n",
        "            #   # if self.wts > self.ta : \n",
        "            #   #   h.partial_fit(features, label, w)  \n",
        "            #   # h=h.partial_fit(features,label,w)\n",
        "            # else:\n",
        "            h.partial_fit(features, w, w)\n",
        "            \n",
        "              \n",
        "                # Update error weight\n",
        "                # if h.predict(features ,self.regression) != label:\n",
        "                    # self.errors[m][k] += w\n",
        "                \n",
        "                # if self.errors[m][k] < min_error:\n",
        "                #     min_error = self.errors[m][k]\n",
        "                #     best_k = k\n",
        "            # print \"m=\",m    \n",
        "            # Representative for selector M is best learner\n",
        "            \n",
        "            # self.f[m] = h\n",
        "            \n",
        "            self.f[m] = h\n",
        "            features=features.reshape(1,-1)\n",
        "            # print \"before\",F[m]\n",
        "            F[m+1] =h.predict(features)\n",
        "            # print \"after\",F[m]\n",
        "            F[m+1] = (1- self.gama * self.sigma[m]) * F[m] + self.gama * F[m+1]\n",
        "            # F[m+1]+=F[m]\n",
        "            # Update weight using loss function\n",
        "            w = - GAOGBooster.dloss(label[0] * F[m+1])\n",
        "            self.w.append(w)\n",
        "            self.wts=w/sum(self.w)\n",
        "            # self.w.append(w)\n",
        "\n",
        "            # update the sigma\n",
        "            mo=1.0/(self.LB*self.B*sqrt(float(self.t)))\n",
        "            for i in range(self.M):\n",
        "                dd=self.sigma[i]-(mo*(-GAOGBooster.dloss(label[0]* F[i]))*F[i])\n",
        "                ddd=abs(min(dd,1))\n",
        "                self.sigma[i]=max(ddd,0)\n",
        "        # print \"sum=\",sum()        \n",
        "        # print \"F=\" ,F[self.M]\n",
        "        # print \"label=\" ,label\n",
        "            \n",
        "            \n",
        "    def predict(self, features):\n",
        "      F=0\n",
        "      i=0\n",
        "      \n",
        "      # print \"sigma pred=\",self.sigma\n",
        "      features=features.reshape(1,-1)\n",
        "      \n",
        "      # for h in self.f :\n",
        "      #   F=gama * h.predict(features) + (1- gama * self.sigma[i])* F\n",
        "      F = sum(h.predict(features) for h in self.f)\n",
        "        # i+=1\n",
        "         \n",
        "      if F > 0:\n",
        "        return 1.0\n",
        "\n",
        "      return 0.0\n",
        "      # p1 = (e ** F) / (1 + e ** F)\n",
        "      # if p1 >= 0.5:\n",
        "      #   return 1.0\n",
        "      # return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx9i2B177wMj"
      },
      "source": [
        "## **OGB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZz4rh-1rANu"
      },
      "source": [
        "\"\"\"\n",
        "    An Online Gradient Boost implementaton from Leistner.\n",
        "\"\"\"\n",
        "\n",
        "from math import log, e ,exp\n",
        "from sys import maxint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class OGBooster(object):\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(x):\n",
        "        return log(1.0 + e ** (-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def dloss(x):\n",
        "        return - 1.0 / (1.0 + e ** x)\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, Learner, classes, M , trials ,alpha,ta,gama):\n",
        "        self.wts=0\n",
        "        self.t=0\n",
        "        self.M = M\n",
        "        self.K = trials\n",
        "        self.learners = [[Learner(classes,alpha,ta) for _ in range(self.K)]\n",
        "                         for _ in range(self.M)]\n",
        "        self.errors = np.zeros((self.M, self.K))\n",
        "        self.w = []\n",
        "        self.f = [learners[0] for learners in self.learners]\n",
        "        # self.regression = LinearRegression() \n",
        "        self.sigma = np.zeros( self.M )\n",
        "        self.gama=gama\n",
        "        self.ta=ta\n",
        "        self.B = min( self.gama * self.M , log( 4.0 / self.gama ) )\n",
        "        self.LB = exp( self.B ) / ( 1 + exp( self.B ) )\n",
        "\n",
        "    def update(self, features, label,k ):\n",
        "        self.K=k\n",
        "        self.t+=1\n",
        "        w = -OGBooster.dloss(0)\n",
        "        F = np.zeros((1+self.M,self.K))\n",
        "        \n",
        "        for m in range(self.M):\n",
        "            \n",
        "            # Track best of the K learners for this selector\n",
        "            best_k = 0\n",
        "            # min_error = maxint\n",
        "            # print \"m=\",m\n",
        "            # for k in range(self.K):\n",
        "            \n",
        "            h = self.learners[m][0]\n",
        "            # if self.t > 2:\n",
        "            #   # if m > 0:\n",
        "            #   #   h1=self.learners[m-1][0]\n",
        "            #   if h.predict(features) != label or label[0]==0:\n",
        "            #     # self.errors[m][0]+=1\n",
        "            #     h.partial_fit(features, label, w) \n",
        "            #   # if self.wts > self.ta : \n",
        "            #   #   h.partial_fit(features, label, w)  \n",
        "            #   # h=h.partial_fit(features,label,w)\n",
        "            # else:\n",
        "              \n",
        "            h.partial_fit(features, label, w)\n",
        "            \n",
        "              \n",
        "                # Update error weight\n",
        "                # if h.predict(features ,self.regression) != label:\n",
        "                    # self.errors[m][k] += w\n",
        "                \n",
        "                # if self.errors[m][k] < min_error:\n",
        "                #     min_error = self.errors[m][k]\n",
        "                #     best_k = k\n",
        "            # print \"m=\",m    \n",
        "            # Representative for selector M is best learner\n",
        "            \n",
        "            # self.f[m] = h\n",
        "            \n",
        "            self.f[m] = h\n",
        "            # features=features\n",
        "            # print \"before\",F[m]\n",
        "            \n",
        "            F[m+1] =h.predict(features)\n",
        "            # print \"after\",F[m]\n",
        "            \n",
        "            # F[m+1] = (1- self.gama * self.sigma[m]) * F[m] + self.gama * F[m+1]\n",
        "            F[m+1]+=F[m]\n",
        "            # Update weight using loss function\n",
        "            w = - OGBooster.dloss(label[0] * F[m+1])\n",
        "            self.w.append(w)\n",
        "            # self.wts=w/sum(self.w)\n",
        "            # self.w.append(w)\n",
        "\n",
        "            # update the sigma\n",
        "            mo=1.0/(self.LB*self.B*sqrt(float(self.t)))\n",
        "            # for i in range(self.M):\n",
        "            #     dd=self.sigma[i]-(mo*(-OGBooster.dloss(label[0]* F[i]))*F[i])\n",
        "            #     ddd=np.zeros(dd.shape)\n",
        "            #     aaa=np.zeros(dd.shape)\n",
        "            #     for ii in range(dd.shape[0]):\n",
        "            #         ddd[ii]=abs(min(dd[ii],1))\n",
        "            #         aaa[ii]=max(ddd[ii],0)\n",
        "            #     print (\"sigma\",self.sigma.shape)\n",
        "            #     print (\"aaa\",aaa.shape)\n",
        "            #     self.sigma[i]=aaa\n",
        "        # print \"sum=\",sum()        \n",
        "        # print \"F=\" ,F[self.M]\n",
        "        # print \"label=\" ,label\n",
        "            \n",
        "            \n",
        "    def predict(self, features):\n",
        "      F=0\n",
        "      i=0\n",
        "      \n",
        "      # print \"sigma pred=\",self.sigma\n",
        "      features=features\n",
        "      \n",
        "      # for h in self.f :\n",
        "      #   F=gama * h.predict(features) + (1- gama * self.sigma[i])* F\n",
        "      F = sum(h.predict(features) for h in self.f)\n",
        "        # i+=1\n",
        "\n",
        "\n",
        "      ff=np.zeros(F.shape)\n",
        "      i=0\n",
        "      for n in F:\n",
        "        if n > 0:\n",
        "          ff[i]=1\n",
        "        i+=1\n",
        "      #   return 1\n",
        "      # else:\n",
        "      return ff\n",
        "\n",
        "\n",
        "      # if F > 0:\n",
        "      #   return 1.0\n",
        "\n",
        "      # return 0.0\n",
        "      # p1 = (e ** F) / (1 + e ** F)\n",
        "      # if p1 >= 0.5:\n",
        "      #   return 1.0\n",
        "      # return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2c5i5HoyarB"
      },
      "source": [
        "## ocp **boost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPafYSYiydbO"
      },
      "source": [
        "\"\"\"\n",
        "    An online boosting algorithm which mixes SmoothBoost with\n",
        "    Online Convex Programming from Chen '12. \n",
        "    Reference:https://github.com/crm416/online_boosting/blob/master/ensemblers/ocpboost.py\n",
        "\"\"\"\n",
        "\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "# from osboost import OSBoost\n",
        "\n",
        "\n",
        "def project(v, z=1.0):\n",
        "    U = set(range(len(v)))\n",
        "    s = 0.0\n",
        "    rho = 0.0\n",
        "\n",
        "    while U:\n",
        "        k = U.pop()\n",
        "\n",
        "        G = set([j for j in U if v[j] >= v[k]])\n",
        "        G.add(k)\n",
        "        L = set([j for j in U if v[j] < v[k]])\n",
        "\n",
        "        delta_rho = len(G)\n",
        "        delta_s = sum(v[j] for j in G)\n",
        "\n",
        "        if (s + delta_s) - (rho + delta_rho) * v[k] < z:\n",
        "            s += delta_s\n",
        "            rho += delta_rho\n",
        "            U = L\n",
        "        else:\n",
        "            G.remove(k)\n",
        "            U = G\n",
        "    theta = (s - z) / rho\n",
        "    return np.array([max(vi - theta, 0) for vi in v])\n",
        "\n",
        "\n",
        "class OCPBoost(OSBoost):\n",
        "\n",
        "    def update(self, features, label):\n",
        "        try:\n",
        "            self.t += 1\n",
        "        except:\n",
        "            self.t = 1\n",
        "\n",
        "        if self.raw_predict(features) * label < self.theta:\n",
        "            eta = 1 / sqrt(self.t)\n",
        "            predictions = np.array([learner.predict(features)\n",
        "                                    for learner in self.learners])\n",
        "            self.alpha += eta * label * predictions\n",
        "            self.alpha = project(self.alpha)\n",
        "\n",
        "        super(OCPBoost, self).update(features, label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4R43lJy1Nd"
      },
      "source": [
        "# **learner**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M-Svs5Ly4vu"
      },
      "source": [
        "## **nb**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt3FrFCyy4Du"
      },
      "source": [
        "from math import log, e\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NaiveBayes(object):\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        if set(classes) != set([-1.0, 1.0]):\n",
        "            raise ValueError\n",
        "\n",
        "        self.sum_w = 0\n",
        "        self.dim = None\n",
        "        self.neg = None\n",
        "        self.pos = None\n",
        "        self.pc = None\n",
        "\n",
        "    def reset(self, x):\n",
        "        self.self_w = 2 * 1e-16\n",
        "        self.dim = len(x)\n",
        "        self.neg = 1e-16 * np.ones((2, self.dim))\n",
        "        self.pos = 1e-16 * np.ones((2, self.dim))\n",
        "        self.pc = 1e-16 * np.ones(2)\n",
        "\n",
        "    def partial_fit(self, x, y, sample_weight=1.0):\n",
        "        if self.dim is None:\n",
        "            self.reset(x)\n",
        "\n",
        "        if y < 1.0:\n",
        "            y = 0\n",
        "        else:\n",
        "            y = 1\n",
        "\n",
        "        self.sum_w += sample_weight\n",
        "        self.pc[y] += sample_weight\n",
        "\n",
        "        for i in range(self.dim):\n",
        "            if x[i] < 1e-15:\n",
        "                self.neg[y][i] += sample_weight\n",
        "            else:\n",
        "                self.pos[y][i] += sample_weight\n",
        "\n",
        "    def raw_predict(self, x):\n",
        "        if self.dim is None:\n",
        "            self.reset(x)\n",
        "\n",
        "        if self.sum_w < 1e-15:\n",
        "            return 0.0\n",
        "\n",
        "        prob = []\n",
        "        for c in (0, 1):\n",
        "            p = self.pc[c] / self.sum_w\n",
        "            if p < 1e-16:\n",
        "                if self.pc[1 - c] / self.sum_w < 1e-15:\n",
        "                    return 0.0\n",
        "                else:\n",
        "                    return 1.0 - 2 * c\n",
        "            p = log(p)\n",
        "            for i in range(self.dim):\n",
        "                if x[i] < 1e-16:\n",
        "                    p += log(self.neg[c][i] / self.pc[c])\n",
        "                else:\n",
        "                    p += log(self.pos[c][i] / self.pc[c])\n",
        "            prob.append(e ** p)\n",
        "        prob[1] = 1.0 / (1.0 + e ** (prob[0] - prob[1]))\n",
        "        return 2.0 * prob[1] - 1.0\n",
        "\n",
        "    def predict(self, x):\n",
        "        #x = np.matrix(x)\n",
        "\tif self.raw_predict(x) > 0.0:\n",
        "            return 1.0\n",
        "        return -1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCUS5Z21zR7W"
      },
      "source": [
        "## **perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1UYHzJwzZRR"
      },
      "source": [
        "\"\"\"\n",
        "    Identical to Perceptron implementation of Chen 2012.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _snorm(w):\n",
        "    return np.linalg.norm(w)\n",
        "\n",
        "\n",
        "class Perceptron(object):\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        if set(classes) != set([-1.0, 1.0]):\n",
        "            raise ValueError\n",
        "        self.w = None\n",
        "\n",
        "    def reset(self, x):\n",
        "        self.w = np.zeros(x.shape)\n",
        "\n",
        "    def raw_predict(self, x):\n",
        "        if self.w is None:\n",
        "            self.reset(x)\n",
        "\n",
        "\t#print (np.matrix(self.w).shape)\n",
        "\t#print (self.w.shape,x.shape, (np.transpose(np.matrix(x)).shape))\n",
        "        prod = np.matrix(self.w) * np.matrix(x).T\n",
        "        norm = _snorm(self.w)\n",
        "\t#print (norm.shape)\n",
        "        if norm > 1.0:\n",
        "            return prod / norm\n",
        "        return prod\n",
        "\n",
        "    def predict(self, x):\n",
        "        #print self.raw_predict(x)\n",
        "        if self.raw_predict(x) > 0.0:\n",
        "            return 1.0\n",
        "        return -1.0\n",
        "\n",
        "    def partial_fit(self, x, y, sample_weight=1.0):\n",
        "        if self.raw_predict(x) * y <= 0.0:\n",
        "            self.w = self.w + sample_weight * y * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKtfvHR5zdU8"
      },
      "source": [
        "## **random stump**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2U-oJBmzlxw"
      },
      "source": [
        "from collections import defaultdict\n",
        "from random import randint\n",
        "\n",
        "\n",
        "class RandomStump(object):\n",
        "\n",
        "    def __init__(self, classes, feature=None):\n",
        "        self.labels = classes\n",
        "        self.label_counts = defaultdict(int)\n",
        "        self.label_sums = defaultdict(int)\n",
        "        self.feature = feature\n",
        "\n",
        "    def partial_fit(self, example, label, sample_weight=1.0):\n",
        "        example = example.reshape(1,-1)\n",
        "        if self.feature is None:\n",
        "            self.feature = randint(0, example.shape[1] - 1)\n",
        "        if label in self.label_sums.keys():\n",
        "            self.label_sums[label] += sample_weight * example[(0, self.feature)]\n",
        "            self.label_counts[label] += sample_weight\n",
        "        else:\n",
        "            self.label_sums[label] = sample_weight * example[(0, self.feature)]\n",
        "            self.label_counts[label] = sample_weight \n",
        "            \n",
        "\n",
        "    def predict(self, x):\n",
        "        x = x.reshape(1,-1)\n",
        "        if not self.label_counts:\n",
        "            return self.labels[0]\n",
        "\n",
        "        def mean(y):\n",
        "            return float(self.label_sums[y]) / self.label_counts[y]\n",
        "\n",
        "        means = [mean(y) for y in self.label_sums.keys()]\n",
        "        if len(means) == 1:\n",
        "            return self.label_sums.keys()[0]\n",
        "\n",
        "        m0 = min(means[1], means[0])\n",
        "        m1 = max(means[1], means[0])\n",
        "        mid = m0 + float(m1 - m0) / 2\n",
        "        if x[(0, self.feature)] < mid:\n",
        "            return self.label_sums.keys()[means.index(m0)]\n",
        "        else:\n",
        "            return self.label_sums.keys()[means.index(m1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLTbB-Gazs7i"
      },
      "source": [
        "## sk_**dicitiontree**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZcvtSLbzxXD"
      },
      "source": [
        "from collections import defaultdict\n",
        "from math import log\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import poisson, seed\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "class DecisionTree(object):\n",
        "\n",
        "\tdef __init__(self, classes):\n",
        "\t\tself.model = DecisionTreeClassifier(max_depth = 1)\n",
        "\t\tself.classes = classes\n",
        "\t\tself.X = None\n",
        "\t\tself.y = None\n",
        "\t\tself.sample_weight = []\n",
        "\t\n",
        "\n",
        "\t'''\n",
        "\tdef fit(self, x, y, sample_weight):\n",
        "\t\tself.model.fit(x,y, sample_weight = self.sample_weight/sum(self.sample_weight) )\n",
        "\t'''\n",
        "\n",
        "\tdef partial_fit(self, X, y, sample_weight =1.0):\n",
        "\t\t\n",
        "\t\tif self.X is None and self.y is None:\n",
        "\t\t\tself.X = X.reshape(1,-1)\n",
        "\t\t\tself.sample_weight.append(sample_weight)\n",
        "\t\t\tself.y = y.reshape(1,-1)\n",
        "\t\telse:\n",
        "\t\t\t#self.X = np.array(x.reshape(1, -1))\n",
        "\t\t\tself.X = np.vstack((self.X, X.reshape(1, -1)))\n",
        "\t\t\t#self.y = y.reshape(1, -1)\n",
        "\t\t\tself.sample_weight.append(sample_weight)\n",
        "\t\t\tself.y = np.vstack((self.y, y.reshape(1, -1)))\n",
        "    \n",
        "\t\twts = [x/sum(self.sample_weight) for x in self.sample_weight]\n",
        "\t\tself.model.fit(self.X,self.y, sample_weight= wts)\n",
        "\n",
        "\tdef predict(self, x):\n",
        "\t\tx = x.reshape(1,-1)\n",
        "\t\ttry:\n",
        "\t\t\ty = self.model.predict(x)[0]\n",
        "\t\texcept:\n",
        "\t\t\treturn self.classes[0]\n",
        "\t\treturn y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW0CrGQMAtsr"
      },
      "source": [
        "## adline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbG2CG-CAx1-"
      },
      "source": [
        "from mlxtend.classifier import Adaline\n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "\n",
        "\n",
        "\n",
        "class ADL(object):\n",
        "\n",
        "  def __init__(self, classes,alpha,ta):\n",
        "    self.model =Adaline(epochs=30,eta=0.01,minibatches=None,random_seed=1)\n",
        "    self.classes = classes\t\n",
        "    self.X =np.array( [[0.5224064,0.,0.,0.,         0.  ,       0.2365671,\n",
        "    0. ,        0.   ,      0.  ,       0.   ,      0.00835305, 0.4400214,\n",
        "    0.1693545 , 0.25552037, 0.38449857 ,0.0941295 , 0.43288094 ,0.03855402,\n",
        "    0.5050047,  0.4735806,  0.05382852, 0.    ,     0.30890673, 0.,\n",
        "    0. ,        0.4054556 , 0.07549285 ,0.6166974 , 0.31639826, 0.37753907,\n",
        "    0.35618162, 0.  ,       0.02731366, 0.40497586, 0.05642596, 0.07270946,\n",
        "    0.04828388, 0.4214782,  0.12557457, 0.13462874, 0.     ,    0.,\n",
        "    0.45177773, 0.16785698 ,0.53299975, 0.13371341 ,0.05088701 ,0.,\n",
        "    0.,         0.43785366, 0.   ,      0.     ,    0.   ,      0.,\n",
        "    0.,         0.    ,     0.3549977 , 0.     ,    0.    ,     0.,\n",
        "    0.03948852 ,0.    ,     0.   ,      0.  ]])   \n",
        "    \n",
        "    self.y = np.array( [1],int)\n",
        "    self.xreplace=np.array([[0.19496976, 0.26862097, 0. ,        0.06796297, 0.22572587, 0.33987999,\n",
        "    0. ,        0.07091141 ,0.1747169 , 0.      ,   0.12796398, 0.07665116,\n",
        "    0.04093431 ,0.07088532, 0.21225952, 0.0918046,  0.10402346, 0.12333339,\n",
        "    0.23017992, 0.      ,   0.   ,      0.03835107 ,0.       ,  0.,\n",
        "    0.00508943, 0.     ,    0.17025623, 0.23656918, 0.      ,   0.14625897,\n",
        "    0.08119091, 0.20471649, 0.16207848 ,0.17155573, 0.10141545 ,0.07925696,\n",
        "    0.16977833, 0.19181441, 0.08977835, 0.    ,     0.   ,      0.,\n",
        "    0.00400347, 0.32179415, 0.04086207, 0.16119584, 0.    ,     0.,\n",
        "    0.  ,       0.07230382, 0.07245548, 0.   ,      0.11368397, 0.,\n",
        "    0.  ,       0.11189204, 0. ,        0.17911172 ,0.  ,       0.13875426,\n",
        "    0.146117,   0.   ,      0.24666138 ,0.   ]])\n",
        "    \n",
        "    self.yreplace = np.array( [0],int)\n",
        "    # #**************************************************************\n",
        "    # #*********************************************************************\n",
        "    # self.xreplace=np.array([[ 0.39578155, 0.39619911, 0.39651164 ,0.39671734 ,0.39681721, 0.39681512,\n",
        "    # 0.3967174  ,0.39653262, 0.39627138 ,0.39594555 ,0.39556801 ,0.3954024,\n",
        "    # 0.39576453 ,0.39606643 ,0.3962954  ,0.3964411 , 0.39649603 ,0.3964555,\n",
        "    # 0.39631787 ,0.39608446, 0.39575937, 0.39534923 ,0.39528534, 0.3953211,\n",
        "    # 0.39532381, 0.39529157 ,0.39522427, 0.39512336 ,0.3949919,  0.39483431,\n",
        "    # 0.39526355]])\n",
        "    # self.yreplace = np.array( [0],int)\n",
        "    # self.X =np.array( [[0.40020561, 0.40026909, 0.40021706, 0.40006092 ,0.39981449, 0.39949277,\n",
        "    # 0.39916205, 0.39915431 ,0.39907151, 0.39890692, 0.3986561 , 0.39831731,\n",
        "    # 0.39789149, 0.39738247, 0.39679661, 0.39702094 ,0.39791575 ,0.39869824,\n",
        "    # 0.39935404, 0.3998729,  0.4002488 , 0.40048006, 0.40056953, 0.40052423,\n",
        "    # 0.4003551,  0.40007642, 0.39970529, 0.39926085, 0.39925408 ,0.39970121,\n",
        "    # 0.40001801]])\n",
        "    # self.y = np.array( [1],int)\n",
        "    self.sample_weight = []\n",
        "\t\t\n",
        "\n",
        "  def partial_fit(self, X, y ,sample_weight):\t\t\n",
        "    \n",
        "    # if self.X.shape[0] is 1 and self.y.shape[0] is 1:\n",
        "    #     if y[0]==1:\n",
        "    #       self.X=self.xreplace\n",
        "    #       self.y=self.yreplace  \n",
        "      \n",
        "    # # else:\n",
        "    # #self.X = np.array(x.reshape(1, -1))\n",
        "    self.X = np.vstack((self.X, X))\n",
        "    #self.y = y.reshape(1, -1)\n",
        "    \n",
        "    # self.sample_weight.append(sample_weight)\n",
        "    \n",
        "    self.y = np.vstack((self.y, y))\n",
        "    \n",
        "    \n",
        "    y_t=np.zeros((self.y.shape[0]))\n",
        "    y_t=self.y[:,0]\n",
        "    y_tt=np.array(y_t, dtype=int)\n",
        "    # wts = [x/sum(self.sample_weight) for x in self.sample_weight]\n",
        "    \n",
        "    \n",
        "    # if self.X.shape[0]<3:\n",
        "    self.X = self.X+0.00001*np.random.rand(self.X.shape[0], self.X.shape[1])\n",
        "\t\t\n",
        "    # wts=sample_weight/sum(self.sample_weight)\n",
        "    # wts=1\n",
        "    \n",
        "    self.model=self.model.fit(self.X , y_tt ) #training the algorithm\n",
        "    \n",
        "\t\t\n",
        "    return self.model\n",
        "\n",
        "  def raw_predict(self, x ):\n",
        "    \n",
        "    # X = x\n",
        "    \n",
        "    y_pred = self.model.predict(x)\n",
        "    \n",
        "    # if y_pred >0:\n",
        "    #   return 1\n",
        "    # else:\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "  def predict(self, x ):\n",
        "    \n",
        "    # X = x.reshape(1,-1)\n",
        "    \n",
        "    y_pred = self.raw_predict(x)\n",
        "    yy=np.zeros(y_pred.shape)\n",
        "    i=0\n",
        "    for n in y_pred:\n",
        "      if n > 0:\n",
        "        yy[i]=1\n",
        "      i+=1\n",
        "      #   return 1\n",
        "      # else:\n",
        "    return yy\n",
        "\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg5HiAkfFeU7"
      },
      "source": [
        "##sk_LR\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsAXtMLdWhx3"
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "# import matplotlib.pyplot as plt  \n",
        "# import seaborn as seabornInstance \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "class LR(object):\n",
        "\n",
        "\tdef __init__(self, classes):\n",
        "\t\tself.regressor = LinearRegression() \n",
        "\t\tself.classes = classes\n",
        "\t\t\n",
        "\t\tself.X = None\n",
        "\t\tself.y = None\n",
        "\t\tself.sample_weight = []\n",
        "\t\t\n",
        "\n",
        "\tdef partial_fit(self, X, y ,sample_weight):\t\t\n",
        "\t\t\n",
        "\t\tif self.X is None and self.y is None:\n",
        "\t\t\tself.X = X.reshape(1,-1)\n",
        "\t\t\tself.sample_weight.append(sample_weight)\n",
        "\t\t\tself.y = y.reshape(1,-1)\n",
        "\t\telse:\n",
        "\t\t\t#self.X = np.array(x.reshape(1, -1))\n",
        "\t\t\tself.X = np.vstack((self.X, X.reshape(1, -1)))\n",
        "\t\t\t#self.y = y.reshape(1, -1)\n",
        "\t\t\tself.sample_weight.append(sample_weight)\n",
        "\t\t\tself.y = np.vstack((self.y, y.reshape(1, -1)))\n",
        "    \n",
        "\t\t# wts = [x/sum(self.sample_weight) for x in self.sample_weight]\n",
        "\t\n",
        "\t\t# for i in range():\n",
        "\t\twts=sample_weight/sum(self.sample_weight)\n",
        "\t\t\n",
        "\t\tself.regression=self.regressor.fit(self.X, self.y ,wts) #training the algorithm\n",
        "\t\t\n",
        "\t\t# # To retrieve the intercept:\n",
        "\t\t# print self.regressor.intercept_\n",
        "\t\t# #For retrieving the slope:\n",
        "\t\t# print self.regressor.coef_\n",
        "\t\t# return self.regression\n",
        "\n",
        "\tdef predict(self, x ):\n",
        "\t\t\n",
        "\t\tX = x.reshape(1,-1)\n",
        "\t\t# print \"x=\",X\n",
        "\t\ty_pred = self.regression.predict(X)\n",
        "\t\tif y_pred >0:\n",
        "\t\t\treturn 1\n",
        "\t\telse:\n",
        "\t\t\treturn 0.0\n",
        "\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC2gOaGIvz0u"
      },
      "source": [
        "## OLR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06X5DlJcv644"
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "# import matplotlib.pyplot as plt  \n",
        "# import seaborn as seabornInstance \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "class OLR(object):\n",
        "\n",
        "  def __init__(self, classes,alpha,ta):\n",
        "    self.regressor = LinearRegression() \n",
        "    self.classes = classes\n",
        "    self.M = 64\n",
        "    self.alpha=alpha\n",
        "    self.ta=ta\n",
        "    self.R = np.random.rand(self.M,self.M)\n",
        "    self.C = np.random.rand(self.M,1)\n",
        "    self.beta=np.zeros((self.M,1))\n",
        "    self.beta=np.dot(self.R , self.C)\n",
        "    self.sample_weight =[]\n",
        "    self.X=None\n",
        "    self.y=None\n",
        "\n",
        "  def partial_fit(self, X, y ,sample_weight):\t\t\n",
        "\t\t\n",
        "    if self.X is None and self.y is None:\n",
        "      kk=1\n",
        "      self.X = X.reshape(1,-1)\n",
        "      self.sample_weight.append(sample_weight)\n",
        "      self.y = y.reshape(1,-1)\n",
        "    # else:\n",
        "    #   kk=0\n",
        "    #   #self.X = np.array(x.reshape(1, -1))\n",
        "    #   self.X = np.vstack((self.X, X.reshape(1, -1)))\n",
        "    #   #self.y = y.reshape(1, -1)\n",
        "    #   self.y = np.vstack((self.y, y.reshape(1, -1)))\n",
        "    #   self.sample_weight.append(sample_weight)\n",
        "      # self.sample_weight = np.lib.pad(self.sample_weight, ((0,1),(0,1)), 'constant', constant_values=(0))\n",
        "      # self.sample_weight[self.sample_weight.shape[0]-1][self.sample_weight.shape[0]-1]=sample_weight\n",
        "\n",
        "    wts = [x/sum(self.sample_weight) for x in self.sample_weight]\n",
        "\t\n",
        "    if wts>=self.ta:\n",
        "        xt=np.transpose(self.X)\n",
        "        a=self.alpha*np.dot(xt,wts)\n",
        "        aa=np.dot(a,self.X)\n",
        "        self.R=self.R+self.alpha*aa\n",
        "        self.C=self.C+self.alpha*np.dot(a,self.y)\n",
        "        self.beta=np.matmul(self.R , self.C) \n",
        "    \n",
        "\n",
        "    return self.beta\n",
        "    # def fit(x):\n",
        "    #   return np.dot(x,self.beta)\n",
        "    #training the algorithm\n",
        "\t\t\n",
        "\t\t# # To retrieve the intercept:\n",
        "\t\t# print self.regressor.intercept_\n",
        "\t\t# #For retrieving the slope:\n",
        "\t\t# print self.regressor.coef_\n",
        "\t\t\n",
        "  def raw_predict(self, x ,beta):\n",
        "\t\t\n",
        "    X = x.reshape(1,-1)\n",
        "    # print \"badi=\",self.beta\n",
        "    # print \"x=\",X\n",
        "    y_pred = np.dot(X,self.beta)\n",
        "    return y_pred\n",
        "\n",
        "  def predict(self,x,beta):\n",
        "\n",
        "    y=self.raw_predict(x,beta)\n",
        "    if y >0:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExzYZxUy0Jae"
      },
      "source": [
        "## **sk_wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm_xyFWo0MoG"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Wrapper(object):\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = x.reshape(1,-1)\n",
        "        try:\n",
        "            return self.model.predict(x)[0]\n",
        "        except:\n",
        "            return self.classes[0]\n",
        "\n",
        "    def partial_fit(self, x, y, sample_weight=1.0):\n",
        "        x = x.reshape(1,-1)\n",
        "        y = y.reshape(1,-1)\n",
        "        w = np.array([sample_weight])\n",
        "        self.model.partial_fit( x, y, classes=self.classes, sample_weight=w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjmjSbwUz6Zi"
      },
      "source": [
        "## sk_**nb**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_scgIrhs0ANf"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# from sk_wrapper import Wrapper\n",
        "\n",
        "\n",
        "class NaiveBayes(Wrapper):\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        self.model = MultinomialNB()\n",
        "        self.classes = classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uS3T9LO0QBf"
      },
      "source": [
        "## sk_**perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egATayla0Xo0"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "\n",
        "class PerceptronClassifier(object):\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "        self.model = Perceptron()\n",
        "        self.w = None\n",
        "        \n",
        "    def predict(self,X):\n",
        "        X = X.reshape(1,-1)\n",
        "        try:\n",
        "            return self.model.predict(X)[0]\n",
        "        except:\n",
        "            return self.classes[0]\n",
        "    \n",
        "    def partial_fit(self, X, y, sample_weight = 1.0):\n",
        "        X = X.reshape(1,-1)\n",
        "        y = y.reshape(1,-1)\n",
        "        return self.model.partial_fit(X,y, sample_weight = [sample_weight], classes = self.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emKTmFqy0aHH"
      },
      "source": [
        "## **nb_gaussian**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5btchm0jCS"
      },
      "source": [
        "from math import log, e, pi\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NaiveBayes(object):\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        if set(classes) != set([-1.0, 1.0]):\n",
        "            raise ValueError\n",
        "\n",
        "        self.sum_w = 0\n",
        "        self.dim = None\n",
        "        self.neg = None\n",
        "        self.pos = None\n",
        "        self.pc = None\n",
        "\n",
        "    def reset(self, x):\n",
        "        self.self_w = 2 * 1e-16\n",
        "        self.dim = x.shape[1]\n",
        "        self.mean = 1e-16 * np.ones((2, self.dim))\n",
        "        self.M2 = 1e-16 * np.ones((2, self.dim))\n",
        "        self.pc = 1e-16 * np.ones(2)\n",
        "\n",
        "    def partial_fit(self, x, y, sample_weight=1.0):\n",
        "        if self.dim is None:\n",
        "            self.reset(x)\n",
        "\n",
        "        if y < 1.0:\n",
        "            y = 0\n",
        "        else:\n",
        "            y = 1\n",
        "\n",
        "        prev_sum_w = self.sum_w\n",
        "        self.sum_w += sample_weight\n",
        "        self.pc[y] += sample_weight\n",
        "\n",
        "        for i in range(self.dim):\n",
        "            v = x[(0, i)]\n",
        "            delta = v - self.mean[y][i]\n",
        "            R = delta * sample_weight / self.sum_w\n",
        "            self.mean[y][i] += R\n",
        "            if prev_sum_w > 0.0:\n",
        "                self.M2[y][i] += prev_sum_w * delta * R\n",
        "\n",
        "    def raw_predict(self, x):\n",
        "        if self.dim is None:\n",
        "            self.reset(x)\n",
        "\n",
        "        if self.sum_w < 1e-15:\n",
        "            return 0.0\n",
        "\n",
        "        prob = []\n",
        "        for c in (0, 1):\n",
        "            p = self.pc[c] / self.sum_w\n",
        "            if p < 1e-15:\n",
        "                if self.pc[1 - c] / self.sum_w < 1e-15:\n",
        "                    return 0.0\n",
        "                else:\n",
        "                    return 1.0 - 2 * c\n",
        "            p = log(p)\n",
        "            mean_c = self.mean[c]\n",
        "            M2_c = self.M2[c]\n",
        "\n",
        "            for i in range(self.dim):\n",
        "                if self.sum_w > 1.0:\n",
        "                    var = M2_c[i] / (self.sum_w - 1.0)\n",
        "                else:\n",
        "                    var = 1e-16\n",
        "                tmp = x[(0, i)] - mean_c[i]\n",
        "                pfc = -0.5 * (log(2 * pi * var) + tmp * tmp / var)\n",
        "                p += pfc\n",
        "\n",
        "            prob.append(p)\n",
        "        prob[1] = 1.0 / (1.0 + e ** (prob[0] - prob[1]))\n",
        "        return 2.0 * prob[1] - 1.0\n",
        "\n",
        "    def predict(self, x):\n",
        "        if self.raw_predict(x) > 0.0:\n",
        "            return 1.0\n",
        "        return -1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZAOVMPX9Me0"
      },
      "source": [
        "# **TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTo15rmL9OqN"
      },
      "source": [
        "from random import shuffle\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class Test():\n",
        "\n",
        "\n",
        "\tdef __init__(self,algorithm, weak_learner,classess, m ,trials,alpha,ta,gama):\n",
        "\t\t# data = zip(X,y)\n",
        "\t\t# self.label0=0\n",
        "\t\t# self.label1=0\n",
        "\t\t\n",
        "\t\tself.classes = classess\n",
        "\t\tK=trials\n",
        "\t\tself.predictor = algorithm(Learner = weak_learner, classes = self.classes, M= m,trials=1,alpha=alpha,ta=ta,gama=gama)\n",
        "\t\tself.correct = 0.0 \n",
        "\t\tself.t = 0\n",
        "\t\tself.baseline = weak_learner(self.classes,alpha,ta)\n",
        "\t\n",
        "\tdef test(self,X,y):\n",
        "\t\tresults = []\n",
        "\t\t# data = zip(X,y)\n",
        "\t\t# shuffle(data)\n",
        "\t\t\n",
        "\t\tresults.append(self.run_test(X,y))\t\n",
        "\t\ta=np.asarray(results)\n",
        "\t\t\n",
        "\t\t# results = zip(*results)\n",
        "\t\t# map(avg, zip(*results[1])))##meangin vorodi &label ro be onvan dad mide\n",
        "\t\t\n",
        "\t\treturn sum(a)/len(a)\n",
        "\t\t\n",
        "\t\n",
        "\n",
        "\n",
        "\n",
        "\tdef run_test(self,X,y):\n",
        "\t\tdata = zip(X,y)\n",
        "\t\tshuffle(data)\n",
        "\t\tperformance_booster = []\n",
        "\t\t# performance_baseline = []\n",
        "\t\t# baseline_correct = 0.0\n",
        "\t\tincorrect_list = []\n",
        "\t\tincorrect = 0.0\n",
        "\t\tfor i in range(13):\n",
        "\t\t\ti1=i+1\n",
        "\t\t\tif i==5:#number of test  data\n",
        "\t\t\t\ti1=i+1\n",
        "\t\t\txxx,X, yyy,Y,XX,YY= load_data(\"eeg\",i1)\n",
        "\t\t# for (X,Y) in data:\n",
        "\t\t\tk=X.shape[0]\n",
        "\t\t\tprint (\"t=\",self.t)\n",
        "\t\t\t\n",
        "\t\t\tself.predictor.update(X,Y,k)\n",
        "\t\t\t\n",
        "\t\t\tyy=self.predictor.predict(X)\n",
        "\t \t\t\n",
        "\t \t\t# if yy==0:\n",
        "\t\t\t# \t self.label0+=1\n",
        "\t\t\t# else:\n",
        "\t\t\t# \tself.label1+=1\n",
        "\t\t\tfor i in range(Y.shape[0]):\n",
        "\t\t\t\tif yy[i] == Y[i]:\n",
        "\t\t\t\t\n",
        "\t\t\t\t\t\tself.correct += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t\tincorrect +=1\n",
        "\t\t\t\tincorrect_list.append(incorrect)\n",
        "\t\t\t\tself.t += 1\n",
        "\t\t\t\tperformance_booster.append(self.correct / self.t)\n",
        "\t \t\tprint (\"accuracy =\", self.correct / self.t)\n",
        "\t\t\t# if self.baseline.predict(X) == Y:\n",
        "\t\t\t# \tbaseline_correct += 1\n",
        "\t\t\t# self.baseline.partial_fit(X,Y)\n",
        "\n",
        "\t\t\t# performance_baseline.append(baseline_correct / self.t)\n",
        "\t\t\t# performance_baseline\n",
        "\t\t# print \"label0_pred=\",self.label0\n",
        "\t\t# print \"label1_pred=\",self.label1\n",
        "\t\treturn performance_booster\n",
        "\n",
        "\tdef final_test(self,X,y):\n",
        "\t\t#print \"Performing Test for\",len(self.predictor),\" weak learners..\"\n",
        "\t\tcorrect_test = 0.0\n",
        "\t\tnum_samples = 0.0\n",
        "\t\tdata = zip(X,y)\n",
        "\t\tcorrect_baseline = 0.0\n",
        "\t\ty_predict=[]\n",
        "\t\t# y_pred=0\n",
        "\t\t# score = -1.0 *np.mean(mean_squared_error(y_genetic, y_pred))\n",
        "\t\t# for (X,y) in data:\n",
        "\t\ty_pred=self.predictor.predict(X)\n",
        "\t\tfor i in range(y.shape[0]):\t\n",
        "\t\t\tif y_pred[i] == y[i]:\n",
        "\t\t\t\tcorrect_test +=1\n",
        "\t\t\tnum_samples +=1\n",
        "\t\t\t# mean_squared_error(y, y_pred)\n",
        "\t\t\ty_predict.append(y_pred)\n",
        "\t \t\t\n",
        "\t\t\t# if self.baseline.predict(X) == y:\n",
        "\t\t\t# \tcorrect_baseline +=1\n",
        "\t\t#print correct_test, correct_baseline\n",
        "\t\t\n",
        "\t\t# score = -1.0 *np.mean(np.asarray(dd))\n",
        "\t\t# print score\n",
        "\t\tavg = float(correct_test)/float(num_samples)\n",
        "\t\t# baseline_avg = float(correct_baseline)/float(num_samples)\n",
        "\t\t# baseline_avg)\n",
        "\t\treturn avg,y_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP9qWNMyiTko"
      },
      "source": [
        "# Genetic Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKrls5imZfZp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e891060f-c6f3-4f21-d35a-6eeaef1186b9"
      },
      "source": [
        "from sklearn import metrics\n",
        "import random\n",
        "\n",
        "def inicialization_populacao_GAOGB(size_GAOGB):\n",
        "  pop =  [[]]*size_GAOGB\n",
        "  # POP=[alpha,ta,gama]\n",
        "  pop = [[random.random(), random.random(), random.uniform(1.0/64,1)] for i in range(0, size_GAOGB)]\n",
        "  return pop\n",
        "\n",
        "# def select(self, population_sorted):\n",
        "#         print \"a9\"\n",
        "#         population_next = []\n",
        "#         for i in range(self.n_best):\n",
        "#             population_next.append(population_sorted[i])\n",
        "#         for i in range(self.n_rand):\n",
        "#             population_next.append(random.choice(population_sorted))\n",
        "#         random.shuffle(population_next)\n",
        "#         return population_next\n",
        "#  def generate(self, population):\n",
        "#         # Selection, crossover and mutation\n",
        "#         print \"a5\"\n",
        "#         scores_sorted, population_sorted = self.fitness(population)\n",
        "#         print \"a8\"\n",
        "#         population = self.select(population_sorted)\n",
        "#         print \"a10\"\n",
        "#         population = self.crossover(population)\n",
        "#         print \"a12\"\n",
        "#         population = self.mutate(population)\n",
        "#         print \"a14\"\n",
        "#         # History\n",
        "#         self.chromosomes_best.append(population_sorted[0])\n",
        "#         self.scores_best.append(scores_sorted[0])\n",
        "#         self.scores_avg.append(np.mean(scores_sorted))\n",
        "#         print \"a15\"\n",
        "#         return population\n",
        "\n",
        "def crossover_GAOGB(mother_1, mother_2):\n",
        "  child = [mother_1[0], mother_2[1], mother_1[2]]    \n",
        "  return child\n",
        "\n",
        "def mutation_GAOGB(child, prob_mut):\n",
        " for c in range(0, len(child)):\n",
        "    if np.random.rand() > prob_mut:\n",
        "      k = randint(0,2)\n",
        "      if child[c][k]>0.5:\n",
        "        child[c][k] = child[c][k] - random.uniform(0,0.5)\n",
        "      else:\n",
        "        child[c][k] = child[c][k] + random.uniform(0,0.5)\n",
        " return child\n",
        "\n",
        "def function_fitness_GAOGB(pop, X_genetic, y_genetic,size_GAOGB): \n",
        "    fitness = {}\n",
        "    M=64\n",
        "    classess=np.unique(np.array([y_genetic]))\n",
        "    for w in pop:\n",
        "        \n",
        "        clf= Test(GAOGBooster,OLR,classess,M,trials=1,alpha=w[0],ta=w[1],gama=w[2])\n",
        "        \n",
        "        train_ac = clf.test(X_genetic,y_genetic)\n",
        "          \n",
        "        test_accuracy,y_predict= clf.final_test(X_genetic,y_genetic)\n",
        "          \n",
        "        y_predict=np.asarray(y_predict)\n",
        "        print \"yy\",y_predict.shape\n",
        "         \n",
        "        fpr, tpr, thresholds = metrics.roc_curve(y_genetic, y_predict, pos_label=2)\n",
        "        fitness[metrics.auc(fpr, tpr)]= [clf, w]\n",
        "        print \"auc\",metrics.auc(fpr, tpr)\n",
        "        print \"fitness\",fitness\n",
        "        # except:\n",
        "        #     pass\n",
        "    return fitness\n",
        "\n",
        "\n",
        "def ag_GAOGB(X_genetic, y_genetic,num_epochs = 1, size_GAOGB=10, prob_mut=0.8):\n",
        "    pop = inicialization_populacao_GAOGB(size_GAOGB)\n",
        "    print \"pop\",pop\n",
        "    fitness = function_fitness_GAOGB(pop , X_genetic, y_genetic, size_GAOGB)\n",
        "    print fitness\n",
        "    pop_fitness_sort = dict(reversed(sorted(fitness.items())))\n",
        "    print pop_fitness_sort\n",
        "    for j in range(0, num_epochs):\n",
        "        #seleciona os pais\n",
        "        print list(pop_fitness_sort.items())[:len(pop_fitness_sort)//2]\n",
        "        print dict(list(pop_fitness_sort.items())[:len(pop_fitness_sort)//2])\n",
        "        print list(dict(list(pop_fitness_sort.items())[:len(pop_fitness_sort)//2]).values())\n",
        "        parent_1 = np.array(list(dict(list(pop_fitness_sort.items())[:len(pop_fitness_sort)//2]).values()))[:, 1]\n",
        "        parent_2 = np.array(list(dict(list(pop_fitness_sort.items())[len(pop_fitness_sort)//2:]).values()))[:, 1]\n",
        "        #cruzamento\n",
        "        child_1 = [crossover_GAOGB(parent_1[i], parent_2[i]) for i in range(0, np.min([len(parent_2), len(parent_1)]))]\n",
        "        child_2 = [crossover_GAOGB(parent_2[i], parent_1[i]) for i in range(0, np.min([len(parent_2), len(parent_1)]))]\n",
        "        child_2 = mutation_GAOGB(child_2, prob_mut)\n",
        "        \n",
        "        #calcula o fitness dos filhos para escolher quem vai passar pra próxima geração\n",
        "        fitness_child_1 = function_fitness_GAOGB(child_1,X_genetic, y_genetic, size_GAOGB)\n",
        "        fitness_child_2 = function_fitness_GAOGB(child_2, X_genetic, y_genetic, size_GAOGB)\n",
        "        pop_fitness_sort.update(fitness_child_1)\n",
        "        pop_fitness_sort.update(fitness_child_2)\n",
        "        sort = dict(reversed(sorted(pop_fitness_sort.items())))\n",
        "        \n",
        "        #seleciona individuos da proxima geração\n",
        "        pop_fitness_sort = dict(list(sort.items())[:size_GAOGB])\n",
        "        best = list(reversed(sorted(pop_fitness_sort.keys())))[0]\n",
        "        best_individual = pop_fitness_sort[best][0]\n",
        "        print (pop_fitness_sort[best][1], best)\n",
        "        \n",
        "    return best_individual\n",
        "\n",
        "best=ag_GAOGB(X_genetic, y_genetic,num_epochs = 5, size_GAOGB=10, prob_mut=0.8)\n",
        "print \"best=\",best"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-51261dfda358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_individual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_GAOGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_genetic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_genetic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_GAOGB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_mut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"best=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_genetic' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3GRUGw20mJ3"
      },
      "source": [
        "# **main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmbwSIgO0yDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d0035e2b-99b4-4dfe-b1b7-4b5fd6d307bd"
      },
      "source": [
        "from random import seed\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import *\n",
        "from mlxtend.classifier import Adaline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import timeit\n",
        "from sklearn.linear_model import LogisticRegression,Perceptron, SGDClassifier\n",
        "from sklearn import  metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "# ============================================================================\n",
        "# ============================================================================\n",
        "# list of algorithm\n",
        "algorithms = {\n",
        "\"smoothboost\":SmoothBoost,\n",
        "\"ogboost\":OGBooster,\n",
        "\"gaogboost\":GAOGBooster,\n",
        "\"osboost\":OSBoost,\n",
        "\"ocpboost\":OCPBoost,\n",
        "\"expboost\":EXPBoost,\n",
        "\"ozaboost\":OzaBoostClassifier}\n",
        "\n",
        "# list of weak learners\n",
        "weak_learners ={\n",
        "\"adline\" : ADL,\n",
        "\"OLR\" : OLR,\n",
        "\"LR\" : LR,\n",
        "\"sk_perceptron\": PerceptronClassifier,\n",
        "\"sk_nb\":NaiveBayes,\n",
        "\"gaussian_nb\":NaiveBayes,\n",
        "\"nb\": NaiveBayes,\n",
        "\"sk_decisiontree\": DecisionTree,\n",
        "\"random_stump\":RandomStump,\n",
        "\"decisiontree\":DecisionTree,\n",
        "\"perceptron\":Perceptron}\n",
        "\n",
        "weak_learner= \"adline\" # Select the weak learner from the weak learners list \n",
        "algorithm=\"ogboost\" # Sealect the ensembler from the algorithm list \n",
        "dataset = \"eeg\" # Select the dataset (\"cancer\" / \"eeg\"/ \"p300\")\n",
        "Xx_train,X_test, yy_train,y_test,X,y= load_data(dataset,6) #LOAD DATA\n",
        "\n",
        "X_train, X_genetic, y_train, y_genetic = train_test_split(Xx_train,yy_train,test_size = 0.08, random_state = 1)\n",
        "\n",
        "trials=1\n",
        "M=X_train.shape[1]\n",
        "# kf = KFold(n_splits=14) # Define the split - into 2 folds \n",
        "# kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
        "# # print(kf) \n",
        "# print \"dataset : \", dataset,X_train.shape,X_test.shape\n",
        "\n",
        "i=0\n",
        "# for train_index, test_index in kf.split(X):\n",
        "#     i+=1\n",
        "#     print \"TRAIN:\", train_index, \"TEST:\", test_index\n",
        "#     X_train, X_test = X[train_index], X[test_index]\n",
        "#     y_train, y_test = y[train_index], y[test_index]\n",
        "#     if i==3:\n",
        "#       break\n",
        "\n",
        "\n",
        "\n",
        "# To compare the results, the data are first trained by the basic classifiers \n",
        "# ====================================================================================\n",
        "# other classifiers\n",
        "# =====================================================================================\n",
        "\n",
        "# DecisionTree\n",
        "clf2= DecisionTreeClassifier(max_depth = 100)\n",
        "clf2.fit(X_train,y_train)\n",
        "y_pred=clf2.predict( X_test)\n",
        "print (\"Accuracy DecisionTree  classifier :\",metrics.accuracy_score(y_test,y_pred)) \n",
        "#*****************************************************************************************\n",
        "# # adaboost and gradient boosting and xgboost\n",
        "# import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
        "# clf = AdaBoostClassifier(n_estimators=100)\n",
        "# clf.fit(X_train, y_train)\n",
        "# y_pred = clf.predict(X_test)\n",
        "# print (\"adaboost=\",accuracy_score(y_test, y_pred))\n",
        "# lr=np.array([0.001,0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\n",
        "l=np.arange(0.01, 1, 0.001).tolist()\n",
        "lr=np.asarray(l)\n",
        "acc=np.zeros(lr.shape[0])\n",
        "for i in range(lr.shape[0]):\n",
        "  gbc = GradientBoostingClassifier(learning_rate=lr[i],n_estimators=100)\n",
        "  gbc.fit(X_train, y_train)\n",
        "  y_pred = gbc.predict(X_test)\n",
        "  acc[i]=accuracy_score(y_test, y_pred)\n",
        "# print (\"gradient boosting\",accuracy_score(y_test, y_pred))\n",
        "print(\"alpha\",lr)\n",
        "print(\"acc\",acc)\n",
        "plt.plot(lr, acc)\n",
        "plt.show()\n",
        "# alphas = np.logspace(-3, -1, 30)\n",
        "\n",
        "# for Model in [Lasso, Ridge]:\n",
        "#     scores = [cross_val_score(Model(alpha), X, y, cv=3).mean()\n",
        "#               for alpha in alphas]\n",
        "#     plt.plot(alphas, scores, label=Model.__name__) \n",
        "\n",
        "\n",
        "# D_train = xgb.DMatrix(X_train, label=y_train)\n",
        "# D_test = xgb.DMatrix(X_test, label=y_test)\n",
        "# param = {\n",
        "#     'eta': 0.3, \n",
        "#     'max_depth': 3,  \n",
        "#     'objective': 'multi:softprob',  \n",
        "#     'num_class': 3} \n",
        "\n",
        "# steps = 20  \n",
        "# model = xgb.train(param, D_train, steps)\n",
        "\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "\n",
        "# preds = model.predict(D_test)\n",
        "# best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "\n",
        "# # print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
        "# # print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
        "# print(\"xgboost = {}\".format(accuracy_score(y_test, best_preds)))\n",
        "\n",
        "\n",
        "# *******************************************************************************\n",
        "\n",
        "# RandomForestClassifier\n",
        "\n",
        "clf_rf = RandomForestClassifier()\n",
        "clf_rf.fit(X_train,y_train)\n",
        "y_pred_rf = clf_rf.predict( X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print (\"Accuracy RandomForest  classifier :  \",acc_rf)\n",
        "# *****************************************************\n",
        "\n",
        "\n",
        "# l=np.arange(0.01, 1, 0.01).tolist()\n",
        "# lr=np.asarray(l)\n",
        "# acc=np.zeros(lr.shape[0])\n",
        "# print (y_train[0])\n",
        "# y_t=np.zeros((y_train.shape[0]))\n",
        "# y_t=y_train[:,0]\n",
        "# y_tt=np.array(y_t, dtype=int)\n",
        "# for i in range(lr.shape[0]):\n",
        "#   ada = Adaline(epochs=30,eta=lr[i],minibatches=None,random_seed=1)\n",
        "#   ada.fit(X_train, y_tt)\n",
        "#   yyy=ada.predict(X_test)\n",
        "#   acc[i]=metrics.accuracy_score(y_test,yyy)\n",
        "# # print (\"gradient boosting\",accuracy_score(y_test, y_pred))\n",
        "# print(\"eta\",lr)\n",
        "# print(\"acc\",acc)\n",
        "# plt.plot(lr, acc)\n",
        "# plt.show()\n",
        "\n",
        "# adline classifier\n",
        "\n",
        "\n",
        "\n",
        "#Confusion matrix, Accuracy, sensitivity and specificity\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm1=confusion_matrix(y_test, yyy)\n",
        "total1=sum(sum(cm1))\n",
        "\n",
        "accuracy1=float(cm1[0][0]+cm1[1][1])/total1\n",
        "\n",
        "score_adline=ada.score(X_test, y_test)\n",
        "sensitivity1 = float(cm1[0][0])/(cm1[0][0]+cm1[0][1])\n",
        "print (\"Sensitivity : \", sensitivity1 )\n",
        "\n",
        "specificity1 = float(cm1[1,1])/(cm1[1][0]+cm1[1][1])\n",
        "print (\"Specificity : \", specificity1)\n",
        "print (\" Accuracy adaptive linear regression classifier : \" ,metrics.accuracy_score(y_test,yyy) )\n",
        "print (\"confusion matrix:\" ,cm1)\n",
        "\n",
        "\n",
        "\n",
        "# **********************************************************************************************\n",
        "\n",
        "# linear regression classifier\n",
        "model1=LinearRegression()\n",
        "X_train.reshape(1,-1)\n",
        "y_train.reshape(1,-1)\n",
        "# X_genetic.reshape(1,-1)\n",
        "# y_genetic.reshape(1,-1)\n",
        "X_test.reshape(1,-1)\n",
        "model2=model1.fit(X_train,y_train)\n",
        "yy=model2.predict(X_test)\n",
        "print (\"Accuracy linear regression classifier : \",model2.intercept_)\n",
        "# ******************************************************************\n",
        "# logistic regression\n",
        "\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X_train,y_train)\n",
        "score = logisticRegr.score(X_test, y_test)\n",
        "print (\"Accuracy logistic regression classifier : \",score)\n",
        "# ******************************************************************\n",
        "\n",
        "# Gaussian Naive Bayes model\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "print (\"Accuracy Gaussian Naive Bayes classifier : \",metrics.accuracy_score(y_test, y_pred))\n",
        "#****************************************************************************************\n",
        "\n",
        "# perceptron classifier\n",
        "modelll = Perceptron()\n",
        "modelll.partial_fit(X_train,y_train,np.unique(np.array([y_train])))\n",
        "y_p=modelll.predict(X_test)\n",
        "print (\"Accuracy perceptron classifier : \",metrics.accuracy_score(y_test, y_p))\n",
        "\n",
        "# *****************************************************************************************\n",
        "# print \"X0\",X_train[0]\n",
        "# print \"X1\",X_train[2]\n",
        "\n",
        "\n",
        "# ============================================================================================\n",
        "# apply the model\n",
        "# ===========================================================================================\n",
        "\n",
        "print (\"Running : \",algorithm , \" with weaklearner : \",weak_learner)\n",
        "alpha=0.1364825253508657\n",
        "# gama=0.25840666308389154\n",
        "gama=1.0/640\n",
        "ta=0.7740881163694082\n",
        "classess=np.unique(np.array([y_train]))\n",
        "# label0=0\n",
        "# label1=0\n",
        "# for i in range(y_test.shape[0]):\n",
        "#   if y_test[i]==0:\n",
        "#     label0+=1\n",
        "#   else:\n",
        "#     label1+=1\n",
        "# print \"label0=\",label0\n",
        "# print \"label1=\",label1\n",
        "\n",
        "model = Test(algorithms[algorithm], weak_learners[weak_learner],classess,M ,trials,alpha,ta,gama)\n",
        "train_ac = model.test(X_train,y_train)\n",
        "train_ac=np.asarray(train_ac)\n",
        "train_accuracy=sum(train_ac)/train_ac.size\n",
        "test_accuracy,y_predict= model.final_test(X_test,y_test)\n",
        "y_predict=np.asarray(y_predict)\n",
        "\n",
        "\n",
        "print (\"total train accuracy : \",train_accuracy)\n",
        "\n",
        "print (\"total test accuracy : \",test_accuracy)\n",
        "\n",
        "# cm1=confusion_matrix(y_test, y_predict)\n",
        "# print (\"confusion matrix:\" ,cm1)\n",
        "# total1=sum(sum(cm1))\n",
        "# #####from confusion matrix calculate accuracy\n",
        "# accuracy1=float(cm1[0][0]+cm1[1][1])/total1\n",
        "# sensitivity1 = float(cm1[0][0])/(cm1[0][0]+cm1[0][1])\n",
        "# print (\"Sensitivity : \", sensitivity1) \n",
        "\n",
        "# specificity1 = float(cm1[1,1])/(cm1[1][0]+cm1[1][1])\n",
        "# print (\"Specificity : \", specificity1)\n",
        "\n",
        "\n",
        "# *********************************\n",
        "# ******************************************\n",
        "# import matplotlib.cm as cm\n",
        "# from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "# import matplotlib.patches as mpatches\n",
        "# import matplotlib.patches as mpatches\n",
        "# plt.scatter(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Accuracy DecisionTree  classifier :', 0.7542706964520368)\n",
            "('alpha', array([0.01 , 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017, 0.018,\n",
            "       0.019, 0.02 , 0.021, 0.022, 0.023, 0.024, 0.025, 0.026, 0.027,\n",
            "       0.028, 0.029, 0.03 , 0.031, 0.032, 0.033, 0.034, 0.035, 0.036,\n",
            "       0.037, 0.038, 0.039, 0.04 , 0.041, 0.042, 0.043, 0.044, 0.045,\n",
            "       0.046, 0.047, 0.048, 0.049, 0.05 , 0.051, 0.052, 0.053, 0.054,\n",
            "       0.055, 0.056, 0.057, 0.058, 0.059, 0.06 , 0.061, 0.062, 0.063,\n",
            "       0.064, 0.065, 0.066, 0.067, 0.068, 0.069, 0.07 , 0.071, 0.072,\n",
            "       0.073, 0.074, 0.075, 0.076, 0.077, 0.078, 0.079, 0.08 , 0.081,\n",
            "       0.082, 0.083, 0.084, 0.085, 0.086, 0.087, 0.088, 0.089, 0.09 ,\n",
            "       0.091, 0.092, 0.093, 0.094, 0.095, 0.096, 0.097, 0.098, 0.099,\n",
            "       0.1  , 0.101, 0.102, 0.103, 0.104, 0.105, 0.106, 0.107, 0.108,\n",
            "       0.109, 0.11 , 0.111, 0.112, 0.113, 0.114, 0.115, 0.116, 0.117,\n",
            "       0.118, 0.119, 0.12 , 0.121, 0.122, 0.123, 0.124, 0.125, 0.126,\n",
            "       0.127, 0.128, 0.129, 0.13 , 0.131, 0.132, 0.133, 0.134, 0.135,\n",
            "       0.136, 0.137, 0.138, 0.139, 0.14 , 0.141, 0.142, 0.143, 0.144,\n",
            "       0.145, 0.146, 0.147, 0.148, 0.149, 0.15 , 0.151, 0.152, 0.153,\n",
            "       0.154, 0.155, 0.156, 0.157, 0.158, 0.159, 0.16 , 0.161, 0.162,\n",
            "       0.163, 0.164, 0.165, 0.166, 0.167, 0.168, 0.169, 0.17 , 0.171,\n",
            "       0.172, 0.173, 0.174, 0.175, 0.176, 0.177, 0.178, 0.179, 0.18 ,\n",
            "       0.181, 0.182, 0.183, 0.184, 0.185, 0.186, 0.187, 0.188, 0.189,\n",
            "       0.19 , 0.191, 0.192, 0.193, 0.194, 0.195, 0.196, 0.197, 0.198,\n",
            "       0.199, 0.2  , 0.201, 0.202, 0.203, 0.204, 0.205, 0.206, 0.207,\n",
            "       0.208, 0.209, 0.21 , 0.211, 0.212, 0.213, 0.214, 0.215, 0.216,\n",
            "       0.217, 0.218, 0.219, 0.22 , 0.221, 0.222, 0.223, 0.224, 0.225,\n",
            "       0.226, 0.227, 0.228, 0.229, 0.23 , 0.231, 0.232, 0.233, 0.234,\n",
            "       0.235, 0.236, 0.237, 0.238, 0.239, 0.24 , 0.241, 0.242, 0.243,\n",
            "       0.244, 0.245, 0.246, 0.247, 0.248, 0.249, 0.25 , 0.251, 0.252,\n",
            "       0.253, 0.254, 0.255, 0.256, 0.257, 0.258, 0.259, 0.26 , 0.261,\n",
            "       0.262, 0.263, 0.264, 0.265, 0.266, 0.267, 0.268, 0.269, 0.27 ,\n",
            "       0.271, 0.272, 0.273, 0.274, 0.275, 0.276, 0.277, 0.278, 0.279,\n",
            "       0.28 , 0.281, 0.282, 0.283, 0.284, 0.285, 0.286, 0.287, 0.288,\n",
            "       0.289, 0.29 , 0.291, 0.292, 0.293, 0.294, 0.295, 0.296, 0.297,\n",
            "       0.298, 0.299, 0.3  , 0.301, 0.302, 0.303, 0.304, 0.305, 0.306,\n",
            "       0.307, 0.308, 0.309, 0.31 , 0.311, 0.312, 0.313, 0.314, 0.315,\n",
            "       0.316, 0.317, 0.318, 0.319, 0.32 , 0.321, 0.322, 0.323, 0.324,\n",
            "       0.325, 0.326, 0.327, 0.328, 0.329, 0.33 , 0.331, 0.332, 0.333,\n",
            "       0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34 , 0.341, 0.342,\n",
            "       0.343, 0.344, 0.345, 0.346, 0.347, 0.348, 0.349, 0.35 , 0.351,\n",
            "       0.352, 0.353, 0.354, 0.355, 0.356, 0.357, 0.358, 0.359, 0.36 ,\n",
            "       0.361, 0.362, 0.363, 0.364, 0.365, 0.366, 0.367, 0.368, 0.369,\n",
            "       0.37 , 0.371, 0.372, 0.373, 0.374, 0.375, 0.376, 0.377, 0.378,\n",
            "       0.379, 0.38 , 0.381, 0.382, 0.383, 0.384, 0.385, 0.386, 0.387,\n",
            "       0.388, 0.389, 0.39 , 0.391, 0.392, 0.393, 0.394, 0.395, 0.396,\n",
            "       0.397, 0.398, 0.399, 0.4  , 0.401, 0.402, 0.403, 0.404, 0.405,\n",
            "       0.406, 0.407, 0.408, 0.409, 0.41 , 0.411, 0.412, 0.413, 0.414,\n",
            "       0.415, 0.416, 0.417, 0.418, 0.419, 0.42 , 0.421, 0.422, 0.423,\n",
            "       0.424, 0.425, 0.426, 0.427, 0.428, 0.429, 0.43 , 0.431, 0.432,\n",
            "       0.433, 0.434, 0.435, 0.436, 0.437, 0.438, 0.439, 0.44 , 0.441,\n",
            "       0.442, 0.443, 0.444, 0.445, 0.446, 0.447, 0.448, 0.449, 0.45 ,\n",
            "       0.451, 0.452, 0.453, 0.454, 0.455, 0.456, 0.457, 0.458, 0.459,\n",
            "       0.46 , 0.461, 0.462, 0.463, 0.464, 0.465, 0.466, 0.467, 0.468,\n",
            "       0.469, 0.47 , 0.471, 0.472, 0.473, 0.474, 0.475, 0.476, 0.477,\n",
            "       0.478, 0.479, 0.48 , 0.481, 0.482, 0.483, 0.484, 0.485, 0.486,\n",
            "       0.487, 0.488, 0.489, 0.49 , 0.491, 0.492, 0.493, 0.494, 0.495,\n",
            "       0.496, 0.497, 0.498, 0.499, 0.5  , 0.501, 0.502, 0.503, 0.504,\n",
            "       0.505, 0.506, 0.507, 0.508, 0.509, 0.51 , 0.511, 0.512, 0.513,\n",
            "       0.514, 0.515, 0.516, 0.517, 0.518, 0.519, 0.52 , 0.521, 0.522,\n",
            "       0.523, 0.524, 0.525, 0.526, 0.527, 0.528, 0.529, 0.53 , 0.531,\n",
            "       0.532, 0.533, 0.534, 0.535, 0.536, 0.537, 0.538, 0.539, 0.54 ,\n",
            "       0.541, 0.542, 0.543, 0.544, 0.545, 0.546, 0.547, 0.548, 0.549,\n",
            "       0.55 , 0.551, 0.552, 0.553, 0.554, 0.555, 0.556, 0.557, 0.558,\n",
            "       0.559, 0.56 , 0.561, 0.562, 0.563, 0.564, 0.565, 0.566, 0.567,\n",
            "       0.568, 0.569, 0.57 , 0.571, 0.572, 0.573, 0.574, 0.575, 0.576,\n",
            "       0.577, 0.578, 0.579, 0.58 , 0.581, 0.582, 0.583, 0.584, 0.585,\n",
            "       0.586, 0.587, 0.588, 0.589, 0.59 , 0.591, 0.592, 0.593, 0.594,\n",
            "       0.595, 0.596, 0.597, 0.598, 0.599, 0.6  , 0.601, 0.602, 0.603,\n",
            "       0.604, 0.605, 0.606, 0.607, 0.608, 0.609, 0.61 , 0.611, 0.612,\n",
            "       0.613, 0.614, 0.615, 0.616, 0.617, 0.618, 0.619, 0.62 , 0.621,\n",
            "       0.622, 0.623, 0.624, 0.625, 0.626, 0.627, 0.628, 0.629, 0.63 ,\n",
            "       0.631, 0.632, 0.633, 0.634, 0.635, 0.636, 0.637, 0.638, 0.639,\n",
            "       0.64 , 0.641, 0.642, 0.643, 0.644, 0.645, 0.646, 0.647, 0.648,\n",
            "       0.649, 0.65 , 0.651, 0.652, 0.653, 0.654, 0.655, 0.656, 0.657,\n",
            "       0.658, 0.659, 0.66 , 0.661, 0.662, 0.663, 0.664, 0.665, 0.666,\n",
            "       0.667, 0.668, 0.669, 0.67 , 0.671, 0.672, 0.673, 0.674, 0.675,\n",
            "       0.676, 0.677, 0.678, 0.679, 0.68 , 0.681, 0.682, 0.683, 0.684,\n",
            "       0.685, 0.686, 0.687, 0.688, 0.689, 0.69 , 0.691, 0.692, 0.693,\n",
            "       0.694, 0.695, 0.696, 0.697, 0.698, 0.699, 0.7  , 0.701, 0.702,\n",
            "       0.703, 0.704, 0.705, 0.706, 0.707, 0.708, 0.709, 0.71 , 0.711,\n",
            "       0.712, 0.713, 0.714, 0.715, 0.716, 0.717, 0.718, 0.719, 0.72 ,\n",
            "       0.721, 0.722, 0.723, 0.724, 0.725, 0.726, 0.727, 0.728, 0.729,\n",
            "       0.73 , 0.731, 0.732, 0.733, 0.734, 0.735, 0.736, 0.737, 0.738,\n",
            "       0.739, 0.74 , 0.741, 0.742, 0.743, 0.744, 0.745, 0.746, 0.747,\n",
            "       0.748, 0.749, 0.75 , 0.751, 0.752, 0.753, 0.754, 0.755, 0.756,\n",
            "       0.757, 0.758, 0.759, 0.76 , 0.761, 0.762, 0.763, 0.764, 0.765,\n",
            "       0.766, 0.767, 0.768, 0.769, 0.77 , 0.771, 0.772, 0.773, 0.774,\n",
            "       0.775, 0.776, 0.777, 0.778, 0.779, 0.78 , 0.781, 0.782, 0.783,\n",
            "       0.784, 0.785, 0.786, 0.787, 0.788, 0.789, 0.79 , 0.791, 0.792,\n",
            "       0.793, 0.794, 0.795, 0.796, 0.797, 0.798, 0.799, 0.8  , 0.801,\n",
            "       0.802, 0.803, 0.804, 0.805, 0.806, 0.807, 0.808, 0.809, 0.81 ,\n",
            "       0.811, 0.812, 0.813, 0.814, 0.815, 0.816, 0.817, 0.818, 0.819,\n",
            "       0.82 , 0.821, 0.822, 0.823, 0.824, 0.825, 0.826, 0.827, 0.828,\n",
            "       0.829, 0.83 , 0.831, 0.832, 0.833, 0.834, 0.835, 0.836, 0.837,\n",
            "       0.838, 0.839, 0.84 , 0.841, 0.842, 0.843, 0.844, 0.845, 0.846,\n",
            "       0.847, 0.848, 0.849, 0.85 , 0.851, 0.852, 0.853, 0.854, 0.855,\n",
            "       0.856, 0.857, 0.858, 0.859, 0.86 , 0.861, 0.862, 0.863, 0.864,\n",
            "       0.865, 0.866, 0.867, 0.868, 0.869, 0.87 , 0.871, 0.872, 0.873,\n",
            "       0.874, 0.875, 0.876, 0.877, 0.878, 0.879, 0.88 , 0.881, 0.882,\n",
            "       0.883, 0.884, 0.885, 0.886, 0.887, 0.888, 0.889, 0.89 , 0.891,\n",
            "       0.892, 0.893, 0.894, 0.895, 0.896, 0.897, 0.898, 0.899, 0.9  ,\n",
            "       0.901, 0.902, 0.903, 0.904, 0.905, 0.906, 0.907, 0.908, 0.909,\n",
            "       0.91 , 0.911, 0.912, 0.913, 0.914, 0.915, 0.916, 0.917, 0.918,\n",
            "       0.919, 0.92 , 0.921, 0.922, 0.923, 0.924, 0.925, 0.926, 0.927,\n",
            "       0.928, 0.929, 0.93 , 0.931, 0.932, 0.933, 0.934, 0.935, 0.936,\n",
            "       0.937, 0.938, 0.939, 0.94 , 0.941, 0.942, 0.943, 0.944, 0.945,\n",
            "       0.946, 0.947, 0.948, 0.949, 0.95 , 0.951, 0.952, 0.953, 0.954,\n",
            "       0.955, 0.956, 0.957, 0.958, 0.959, 0.96 , 0.961, 0.962, 0.963,\n",
            "       0.964, 0.965, 0.966, 0.967, 0.968, 0.969, 0.97 , 0.971, 0.972,\n",
            "       0.973, 0.974, 0.975, 0.976, 0.977, 0.978, 0.979, 0.98 , 0.981,\n",
            "       0.982, 0.983, 0.984, 0.985, 0.986, 0.987, 0.988, 0.989, 0.99 ,\n",
            "       0.991, 0.992, 0.993, 0.994, 0.995, 0.996, 0.997, 0.998, 0.999]))\n",
            "('acc', array([0.82654402, 0.82785808, 0.82260184, 0.82654402, 0.8304862 ,\n",
            "       0.83311432, 0.83311432, 0.83311432, 0.83574244, 0.83180026,\n",
            "       0.8370565 , 0.83574244, 0.83574244, 0.83442838, 0.83311432,\n",
            "       0.83180026, 0.83442838, 0.83311432, 0.83574244, 0.8370565 ,\n",
            "       0.83574244, 0.8370565 , 0.83574244, 0.8370565 , 0.8370565 ,\n",
            "       0.83574244, 0.83442838, 0.8370565 , 0.8370565 , 0.83837057,\n",
            "       0.83574244, 0.8370565 , 0.83837057, 0.83442838, 0.83968463,\n",
            "       0.83837057, 0.8370565 , 0.83574244, 0.84099869, 0.84099869,\n",
            "       0.84362681, 0.83837057, 0.83574244, 0.83837057, 0.8370565 ,\n",
            "       0.84099869, 0.83968463, 0.84099869, 0.84099869, 0.84099869,\n",
            "       0.83837057, 0.8370565 , 0.83968463, 0.83968463, 0.83837057,\n",
            "       0.83968463, 0.84099869, 0.84362681, 0.84362681, 0.84231275,\n",
            "       0.8370565 , 0.83837057, 0.8370565 , 0.83837057, 0.84099869,\n",
            "       0.83968463, 0.83968463, 0.84231275, 0.8370565 , 0.84362681,\n",
            "       0.84099869, 0.84494087, 0.84625493, 0.83574244, 0.84362681,\n",
            "       0.84099869, 0.84099869, 0.83968463, 0.84231275, 0.84888305,\n",
            "       0.83837057, 0.83968463, 0.84362681, 0.83968463, 0.84362681,\n",
            "       0.8370565 , 0.83837057, 0.84231275, 0.84362681, 0.84362681,\n",
            "       0.8370565 , 0.84362681, 0.84099869, 0.84362681, 0.83968463,\n",
            "       0.83837057, 0.84362681, 0.84099869, 0.83968463, 0.83968463,\n",
            "       0.83968463, 0.84231275, 0.84625493, 0.84362681, 0.84099869,\n",
            "       0.83311432, 0.83837057, 0.83180026, 0.83311432, 0.83968463,\n",
            "       0.83442838, 0.83574244, 0.84362681, 0.83180026, 0.83968463,\n",
            "       0.84494087, 0.84231275, 0.83968463, 0.83837057, 0.84888305,\n",
            "       0.84494087, 0.84099869, 0.84231275, 0.84231275, 0.83968463,\n",
            "       0.8370565 , 0.8370565 , 0.84494087, 0.84231275, 0.84099869,\n",
            "       0.84231275, 0.83574244, 0.83968463, 0.83574244, 0.8370565 ,\n",
            "       0.83311432, 0.8370565 , 0.84362681, 0.84099869, 0.83837057,\n",
            "       0.84099869, 0.83837057, 0.8370565 , 0.84231275, 0.84756899,\n",
            "       0.83837057, 0.84231275, 0.83837057, 0.83574244, 0.83837057,\n",
            "       0.8370565 , 0.84231275, 0.84099869, 0.84231275, 0.83968463,\n",
            "       0.83968463, 0.83837057, 0.84231275, 0.84099869, 0.83574244,\n",
            "       0.8370565 , 0.83574244, 0.84099869, 0.84099869, 0.83968463,\n",
            "       0.84362681, 0.83442838, 0.84231275, 0.8370565 , 0.83837057,\n",
            "       0.83442838, 0.83311432, 0.83442838, 0.83837057, 0.84362681,\n",
            "       0.8370565 , 0.8370565 , 0.83442838, 0.83180026, 0.83180026,\n",
            "       0.83574244, 0.84231275, 0.83311432, 0.82785808, 0.83837057,\n",
            "       0.82785808, 0.83311432, 0.8370565 , 0.82917214, 0.82522996,\n",
            "       0.82654402, 0.82917214, 0.82785808, 0.82785808, 0.82917214,\n",
            "       0.83311432, 0.82785808, 0.8304862 , 0.83837057, 0.83837057,\n",
            "       0.84099869, 0.8304862 , 0.82917214, 0.82917214, 0.83442838,\n",
            "       0.82785808, 0.83837057, 0.83311432, 0.84362681, 0.83311432,\n",
            "       0.8370565 , 0.8370565 , 0.83180026, 0.82917214, 0.83311432,\n",
            "       0.82917214, 0.8370565 , 0.82654402, 0.82654402, 0.83837057,\n",
            "       0.83311432, 0.83311432, 0.8370565 , 0.83574244, 0.8370565 ,\n",
            "       0.84231275, 0.83442838, 0.83574244, 0.8370565 , 0.82917214,\n",
            "       0.84231275, 0.83442838, 0.81997372, 0.83442838, 0.81865966,\n",
            "       0.82917214, 0.82785808, 0.8370565 , 0.8304862 , 0.83311432,\n",
            "       0.83180026, 0.82654402, 0.83837057, 0.83442838, 0.83180026,\n",
            "       0.82785808, 0.83311432, 0.82917214, 0.83442838, 0.83574244,\n",
            "       0.82917214, 0.84099869, 0.83574244, 0.8370565 , 0.84625493,\n",
            "       0.84362681, 0.83180026, 0.83968463, 0.83180026, 0.83311432,\n",
            "       0.8304862 , 0.82917214, 0.83442838, 0.83311432, 0.83180026,\n",
            "       0.83311432, 0.83837057, 0.8304862 , 0.82917214, 0.84362681,\n",
            "       0.83442838, 0.83968463, 0.82917214, 0.83180026, 0.82917214,\n",
            "       0.83180026, 0.82917214, 0.83442838, 0.81997372, 0.83837057,\n",
            "       0.82785808, 0.82522996, 0.82260184, 0.84099869, 0.82785808,\n",
            "       0.84362681, 0.83574244, 0.82917214, 0.83180026, 0.83311432,\n",
            "       0.8304862 , 0.83180026, 0.8304862 , 0.84231275, 0.83837057,\n",
            "       0.82917214, 0.83180026, 0.82917214, 0.82128778, 0.82654402,\n",
            "       0.83311432, 0.82128778, 0.83442838, 0.83180026, 0.8173456 ,\n",
            "       0.81865966, 0.82260184, 0.83180026, 0.8239159 , 0.83837057,\n",
            "       0.82785808, 0.8239159 , 0.8370565 , 0.82917214, 0.82917214,\n",
            "       0.8370565 , 0.83574244, 0.82522996, 0.8304862 , 0.83180026,\n",
            "       0.82654402, 0.8239159 , 0.82260184, 0.83574244, 0.82917214,\n",
            "       0.83311432, 0.8304862 , 0.82654402, 0.8173456 , 0.8304862 ,\n",
            "       0.83574244, 0.83574244, 0.82917214, 0.83311432, 0.82917214,\n",
            "       0.83180026, 0.83968463, 0.83311432, 0.8304862 , 0.83311432,\n",
            "       0.83311432, 0.83311432, 0.82260184, 0.83311432, 0.82785808,\n",
            "       0.81865966, 0.8304862 , 0.82917214, 0.8304862 , 0.83180026,\n",
            "       0.83180026, 0.84231275, 0.83574244, 0.8239159 , 0.82654402,\n",
            "       0.82785808, 0.83574244, 0.83442838, 0.8239159 , 0.84099869,\n",
            "       0.81471748, 0.82522996, 0.83574244, 0.83574244, 0.82917214,\n",
            "       0.81997372, 0.82917214, 0.81603154, 0.83442838, 0.83311432,\n",
            "       0.82785808, 0.8239159 , 0.83180026, 0.83442838, 0.81340342,\n",
            "       0.83311432, 0.82128778, 0.8370565 , 0.83442838, 0.8239159 ,\n",
            "       0.82785808, 0.81603154, 0.8173456 , 0.81603154, 0.83311432,\n",
            "       0.8304862 , 0.81865966, 0.8239159 , 0.81340342, 0.82260184,\n",
            "       0.81865966, 0.8173456 , 0.82654402, 0.8173456 , 0.8107753 ,\n",
            "       0.81865966, 0.8239159 , 0.82654402, 0.81997372, 0.81997372,\n",
            "       0.8304862 , 0.8304862 , 0.82917214, 0.82128778, 0.83180026,\n",
            "       0.81865966, 0.83180026, 0.82785808, 0.81865966, 0.80420499,\n",
            "       0.82260184, 0.8107753 , 0.83311432, 0.82128778, 0.82917214,\n",
            "       0.81997372, 0.8239159 , 0.82128778, 0.82654402, 0.82522996,\n",
            "       0.8239159 , 0.82654402, 0.81997372, 0.8304862 , 0.81340342,\n",
            "       0.82785808, 0.81997372, 0.81340342, 0.82128778, 0.82128778,\n",
            "       0.8239159 , 0.82522996, 0.82522996, 0.81865966, 0.82522996,\n",
            "       0.82128778, 0.81997372, 0.81471748, 0.8173456 , 0.81208936,\n",
            "       0.80551905, 0.8173456 , 0.82260184, 0.8239159 , 0.8239159 ,\n",
            "       0.81997372, 0.82522996, 0.82128778, 0.83837057, 0.82522996,\n",
            "       0.80683311, 0.81340342, 0.80157687, 0.80946124, 0.81340342,\n",
            "       0.80946124, 0.80551905, 0.81471748, 0.81865966, 0.82785808,\n",
            "       0.81603154, 0.82128778, 0.81865966, 0.8107753 , 0.82128778,\n",
            "       0.80157687, 0.8239159 , 0.82917214, 0.82654402, 0.81340342,\n",
            "       0.8173456 , 0.81603154, 0.82917214, 0.81865966, 0.82785808,\n",
            "       0.82654402, 0.81997372, 0.80026281, 0.81208936, 0.8173456 ,\n",
            "       0.80814717, 0.8107753 , 0.8173456 , 0.81340342, 0.82654402,\n",
            "       0.81208936, 0.82522996, 0.81865966, 0.81997372, 0.81340342,\n",
            "       0.8107753 , 0.81865966, 0.80946124, 0.82522996, 0.80551905,\n",
            "       0.8173456 , 0.81340342, 0.8107753 , 0.81865966, 0.80814717,\n",
            "       0.82917214, 0.82522996, 0.81997372, 0.80289093, 0.80946124,\n",
            "       0.81603154, 0.82785808, 0.8173456 , 0.82522996, 0.82654402,\n",
            "       0.80683311, 0.81340342, 0.81340342, 0.81340342, 0.82654402,\n",
            "       0.83574244, 0.81471748, 0.8173456 , 0.80946124, 0.82128778,\n",
            "       0.81997372, 0.8304862 , 0.80157687, 0.8173456 , 0.82128778,\n",
            "       0.8107753 , 0.8107753 , 0.8239159 , 0.82260184, 0.81865966,\n",
            "       0.80946124, 0.82785808, 0.82128778, 0.8239159 , 0.82128778,\n",
            "       0.81603154, 0.8304862 , 0.81471748, 0.8173456 , 0.82128778,\n",
            "       0.8173456 , 0.82260184, 0.8173456 , 0.81208936, 0.80814717,\n",
            "       0.82522996, 0.80814717, 0.80420499, 0.82260184, 0.81471748,\n",
            "       0.82785808, 0.81603154, 0.81865966, 0.80026281, 0.81471748,\n",
            "       0.80814717, 0.80946124, 0.80157687, 0.8239159 , 0.82128778,\n",
            "       0.8173456 , 0.81340342, 0.81208936, 0.81208936, 0.81603154,\n",
            "       0.81471748, 0.80814717, 0.80814717, 0.80946124, 0.80026281,\n",
            "       0.81865966, 0.82260184, 0.81603154, 0.80683311, 0.80683311,\n",
            "       0.79894875, 0.80420499, 0.80157687, 0.80946124, 0.81865966,\n",
            "       0.81865966, 0.81997372, 0.79632063, 0.78580815, 0.80157687,\n",
            "       0.81865966, 0.82260184, 0.79894875, 0.79763469, 0.80683311,\n",
            "       0.79632063, 0.80814717, 0.81471748, 0.80814717, 0.8107753 ,\n",
            "       0.80420499, 0.79632063, 0.80157687, 0.79369251, 0.80289093,\n",
            "       0.80683311, 0.80157687, 0.81340342, 0.79763469, 0.80289093,\n",
            "       0.81340342, 0.81997372, 0.80683311, 0.80683311, 0.80814717,\n",
            "       0.80551905, 0.80814717, 0.80420499, 0.80157687, 0.79894875,\n",
            "       0.80946124, 0.78580815, 0.78843627, 0.8173456 , 0.8107753 ,\n",
            "       0.79632063, 0.81865966, 0.8173456 , 0.80289093, 0.8173456 ,\n",
            "       0.8239159 , 0.81997372, 0.80289093, 0.80551905, 0.79894875,\n",
            "       0.79632063, 0.79500657, 0.78975033, 0.78975033, 0.80289093,\n",
            "       0.80551905, 0.80289093, 0.79894875, 0.80814717, 0.80157687,\n",
            "       0.79500657, 0.79106439, 0.79894875, 0.79369251, 0.80683311,\n",
            "       0.80814717, 0.79763469, 0.80157687, 0.79237845, 0.80551905,\n",
            "       0.80946124, 0.81340342, 0.79369251, 0.8107753 , 0.8173456 ,\n",
            "       0.78975033, 0.80026281, 0.80026281, 0.81471748, 0.8173456 ,\n",
            "       0.81603154, 0.79500657, 0.80814717, 0.80946124, 0.8173456 ,\n",
            "       0.80157687, 0.80289093, 0.8173456 , 0.81340342, 0.80551905,\n",
            "       0.79632063, 0.80420499, 0.8107753 , 0.80289093, 0.80551905,\n",
            "       0.80420499, 0.80157687, 0.81865966, 0.79237845, 0.79369251,\n",
            "       0.79894875, 0.79763469, 0.80026281, 0.78318003, 0.80946124,\n",
            "       0.8107753 , 0.80814717, 0.80814717, 0.78055191, 0.8239159 ,\n",
            "       0.80814717, 0.80551905, 0.79500657, 0.79369251, 0.78580815,\n",
            "       0.8173456 , 0.79106439, 0.80157687, 0.80157687, 0.80683311,\n",
            "       0.80814717, 0.79106439, 0.81208936, 0.81471748, 0.80814717,\n",
            "       0.80551905, 0.82128778, 0.80551905, 0.80946124, 0.80814717,\n",
            "       0.79106439, 0.79369251, 0.80157687, 0.78712221, 0.80683311,\n",
            "       0.79237845, 0.82260184, 0.78712221, 0.80289093, 0.80026281,\n",
            "       0.79500657, 0.81471748, 0.80814717, 0.80026281, 0.80289093,\n",
            "       0.80026281, 0.80814717, 0.79632063, 0.80814717, 0.78186597,\n",
            "       0.79237845, 0.80420499, 0.81340342, 0.78843627, 0.79894875,\n",
            "       0.79369251, 0.78975033, 0.80551905, 0.80157687, 0.80420499,\n",
            "       0.80814717, 0.80946124, 0.80420499, 0.80026281, 0.80946124,\n",
            "       0.78449409, 0.79763469, 0.80289093, 0.80289093, 0.80289093,\n",
            "       0.80683311, 0.80814717, 0.82522996, 0.80683311, 0.82128778,\n",
            "       0.79500657, 0.80289093, 0.79632063, 0.80814717, 0.79500657,\n",
            "       0.8107753 , 0.80289093, 0.80814717, 0.79763469, 0.80157687,\n",
            "       0.81208936, 0.80814717, 0.8107753 , 0.80551905, 0.79894875,\n",
            "       0.79500657, 0.80551905, 0.80551905, 0.78449409, 0.81865966,\n",
            "       0.81865966, 0.80683311, 0.80289093, 0.80551905, 0.8107753 ,\n",
            "       0.8107753 , 0.79763469, 0.79763469, 0.78843627, 0.80026281,\n",
            "       0.79106439, 0.79763469, 0.78975033, 0.79237845, 0.79106439,\n",
            "       0.80289093, 0.80683311, 0.78975033, 0.77660972, 0.78449409,\n",
            "       0.79632063, 0.78712221, 0.81340342, 0.78318003, 0.79237845,\n",
            "       0.79237845, 0.80814717, 0.80157687, 0.80157687, 0.78712221,\n",
            "       0.77923784, 0.78318003, 0.78975033, 0.80026281, 0.77923784,\n",
            "       0.77923784, 0.80026281, 0.79894875, 0.80289093, 0.81208936,\n",
            "       0.79894875, 0.80946124, 0.79369251, 0.80026281, 0.78712221,\n",
            "       0.79106439, 0.78975033, 0.78975033, 0.79237845, 0.80026281,\n",
            "       0.79500657, 0.77135348, 0.78712221, 0.77923784, 0.78449409,\n",
            "       0.78580815, 0.78449409, 0.79369251, 0.79500657, 0.80683311,\n",
            "       0.78843627, 0.79632063, 0.80026281, 0.79106439, 0.79894875,\n",
            "       0.80157687, 0.80946124, 0.79632063, 0.78186597, 0.77792378,\n",
            "       0.77792378, 0.80289093, 0.80420499, 0.79632063, 0.79894875,\n",
            "       0.79237845, 0.78318003, 0.80551905, 0.78055191, 0.77923784,\n",
            "       0.78186597, 0.78186597, 0.78186597, 0.80026281, 0.78712221,\n",
            "       0.78712221, 0.80026281, 0.80157687, 0.78449409, 0.78712221,\n",
            "       0.78055191, 0.79500657, 0.79500657, 0.79894875, 0.78318003,\n",
            "       0.78449409, 0.80157687, 0.78975033, 0.79369251, 0.80157687,\n",
            "       0.79763469, 0.77923784, 0.78712221, 0.79500657, 0.80420499,\n",
            "       0.78712221, 0.78975033, 0.78055191, 0.78055191, 0.78186597,\n",
            "       0.78055191, 0.79106439, 0.77792378, 0.77923784, 0.77266754,\n",
            "       0.78055191, 0.77792378, 0.80551905, 0.79237845, 0.79894875,\n",
            "       0.78712221, 0.78712221, 0.79894875, 0.79632063, 0.79369251,\n",
            "       0.80814717, 0.80814717, 0.80683311, 0.78186597, 0.79106439,\n",
            "       0.80289093, 0.80683311, 0.80289093, 0.80420499, 0.78843627,\n",
            "       0.79500657, 0.79894875, 0.78712221, 0.80683311, 0.80683311,\n",
            "       0.78843627, 0.79894875, 0.79894875, 0.77266754, 0.79369251,\n",
            "       0.78843627, 0.78580815, 0.80683311, 0.80683311, 0.79106439,\n",
            "       0.78712221, 0.79369251, 0.78580815, 0.79369251, 0.80420499,\n",
            "       0.78712221, 0.79894875, 0.77660972, 0.79894875, 0.81208936,\n",
            "       0.79894875, 0.79763469, 0.79237845, 0.78186597, 0.79369251,\n",
            "       0.79106439, 0.78449409, 0.78843627, 0.79369251, 0.77923784,\n",
            "       0.79894875, 0.79894875, 0.80420499, 0.81865966, 0.78843627,\n",
            "       0.78975033, 0.79106439, 0.78975033, 0.78186597, 0.77135348,\n",
            "       0.78843627, 0.78580815, 0.78055191, 0.79237845, 0.79237845,\n",
            "       0.77135348, 0.79500657, 0.79632063, 0.79369251, 0.78055191,\n",
            "       0.77792378, 0.7739816 , 0.7674113 , 0.78843627, 0.77923784,\n",
            "       0.77923784, 0.78843627, 0.78580815, 0.77266754, 0.79763469,\n",
            "       0.79632063, 0.78712221, 0.79763469, 0.79894875, 0.79894875,\n",
            "       0.78843627, 0.78449409, 0.79106439, 0.78318003, 0.78186597,\n",
            "       0.77792378, 0.78055191, 0.78843627, 0.78843627, 0.78186597]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhxDRFwRByCFE9AVBEHIIEX1BEIQc4v8DxUitiGnHxx4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:138: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Accuracy RandomForest  classifier :  ', 0.812089356110381)\n",
            "('Sensitivity : ', 0.8011363636363636)\n",
            "('Specificity : ', 0.8484107579462102)\n",
            "(' Accuracy adaptive linear regression classifier : ', 0.8265440210249672)\n",
            "('confusion matrix:', array([[282,  70],\n",
            "       [ 62, 347]]))\n",
            "('Accuracy linear regression classifier : ', array([0.5491058], dtype=float32))\n",
            "('Accuracy logistic regression classifier : ', 0.8318002628120894)\n",
            "('Accuracy Gaussian Naive Bayes classifier : ', 0.80946123521682)\n",
            "('Accuracy perceptron classifier : ', 0.6438896189224704)\n",
            "('Running : ', 'ogboost', ' with weaklearner : ', 'adline')\n",
            "('t=', 0)\n",
            "('accuracy =', 0.8478964401294499)\n",
            "('t=', 309)\n",
            "('accuracy =', 0.8446771378708552)\n",
            "('t=', 573)\n",
            "('accuracy =', 0.7703862660944206)\n",
            "('t=', 932)\n",
            "('accuracy =', 0.7933655839668279)\n",
            "('t=', 1447)\n",
            "('accuracy =', 0.796594982078853)\n",
            "('t=', 2232)\n",
            "('accuracy =', 0.8052121617106582)\n",
            "('t=', 2993)\n",
            "('accuracy =', 0.8327244258872651)\n",
            "('t=', 3832)\n",
            "('accuracy =', 0.8232879671635343)\n",
            "('t=', 4629)\n",
            "('accuracy =', 0.8434963186709458)\n",
            "('t=', 5297)\n",
            "('accuracy =', 0.8307692307692308)\n",
            "('t=', 6045)\n",
            "('accuracy =', 0.8298718515245249)\n",
            "('t=', 6789)\n",
            "('accuracy =', 0.8286448471733838)\n",
            "('t=', 7394)\n",
            "('accuracy =', 0.829978813559322)\n",
            "('total train accuracy : ', 0.8196095490687386)\n",
            "('total test accuracy : ', 0.835742444152431)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2scSK4c68af"
      },
      "source": [
        "# plot1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bugql-dor9y"
      },
      "source": [
        "# import numpy as np \n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt \n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt \n",
        "\n",
        "# from matplotlib.colors import ListedColormap\n",
        "\n",
        "# \"\"\" Function to color the deciosion regions \"\"\"\n",
        "\n",
        "# def plot_decision_regions(X, y,x0,x1, classifier, resolution = 0.02):\n",
        "\n",
        "#         # setup marker generator and color map\n",
        "\n",
        "#         markers = ('s', 'x', 'o', '^', 'v')\n",
        "#         colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "#         cmap = ListedColormap(colors[ : len(np.unique(y))])\n",
        "\n",
        "#         # plot the decision surface\n",
        "\n",
        "#         x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "#         x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "#         xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "#           np.arange(x2_min, x2_max, resolution))\n",
        "\n",
        "#         z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "#         Z = z.reshape(xx1.shape)\n",
        "\n",
        "#         plt.contourf(xx1, xx2, Z, alpha = 0.4, cmap = cmap)\n",
        "#         plt.xlim(xx1.min(), xx1.max())\n",
        "#         plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "#         # plot class samples\n",
        "#         # print (\"yy\",np.unique(y))\n",
        "#         # for idx, cl in enumerate(np.unique(y)):\n",
        "#         #   print (idx,cl)\n",
        "#         plt.scatter(x0[:,0], x0[:,1],alpha = 0.8, c = cmap(0), marker = markers[0],\n",
        "#           label = 0)\n",
        "#         plt.scatter(x1[:, 0], x1[:, 1],alpha = 0.8, c = cmap(1), marker = markers[1],\n",
        "#           label = 1)\n",
        "#           # plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
        "#           #   alpha = 0.8, c = cmap(idx), marker = markers[idx],\n",
        "#           #   label = cl)\n",
        "\n",
        "# # from adalinegd import AdalineGD\n",
        "# # # from adalinesgd import AdalineSGD\n",
        "# # import pdr\n",
        "\n",
        "\n",
        "# # get the iris data\n",
        "# # df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
        "# # \theader = None)\n",
        "# x=X\n",
        "# label0=0\n",
        "# label1=0\n",
        "# for i in range(X.shape[0]):\n",
        "#   if y[i][0]==0:\n",
        "#     label0+=1\n",
        "#   else:\n",
        "#     label1+=1\n",
        "# x0=np.zeros((label0,64))\n",
        "# x1=np.zeros((label1,64))\n",
        "# # Plot 100 samples of the data\n",
        "# # y = df.iloc[0:100, 4].values\n",
        "# # # y = np.where(y == 'Iris-setosa', -1, 1)\n",
        "# # X = df.iloc[0:100, [0, 2]].values\n",
        "# ii=0\n",
        "# iii=0\n",
        "# for i in range(X.shape[0]):\n",
        "#   if y[i][0] == 0:\n",
        "#     x0[ii,:]=x[i,:]\n",
        "#     ii+=1\n",
        "#   else:\n",
        "#     x1[iii,:]=x[i,:]\n",
        "#     iii+=1\n",
        "\n",
        "# plt.scatter(x0[:,0], x0[:,1],color ='blue', marker = 'o', label = 'setosa')\n",
        "# plt.scatter(x1[:, 0], x1[:, 1], color ='red', marker = 'o', label = 'setosa')\n",
        "# # plt.scatter(X[:, 0], X[50:100, 1], color = 'blue', marker = 'x', label = 'versicolor')\n",
        "# plt.xlabel('petal length')\n",
        "# plt.ylabel('sepal length')\n",
        "# plt.legend(loc = 'upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Standardize the data\n",
        "# # X_std = np.copy(X)\n",
        "# X_std =np.zeros((X.shape[0],2))\n",
        "# X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
        "# X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()\n",
        "\n",
        "# # Create the AdalineGD model\n",
        "# model1 = Adaline(epochs=30,eta=0.01,minibatches=None,random_seed=1)\n",
        "# y_t=np.zeros((y.shape[0]))\n",
        "# y_t=y[:,0]\n",
        "# y_tt=np.array(y_t, dtype=int)\n",
        "# model1.fit(X_std, y_tt)\n",
        "# # yyy=ada.predict(X_test)\n",
        "\n",
        "\n",
        "# # Plot the training error\n",
        "# plt.plot(range(1, len(model1.cost_) + 1), model1.cost_, marker = 'o', color = 'red')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Sum-squared-error')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot the decision boundary\n",
        "# plot_decision_regions(X_std, y,x0,x1, classifier = model1)\n",
        "# plt.title('Adaptive linear regression ')\n",
        "# plt.xlabel('sepal length [standardized')\n",
        "# plt.ylabel('petal length [standardized]')\n",
        "# plt.legend(loc = 'upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Create the AdalineSGD model\n",
        "# # model2 = AdalineSGD(n_iter = 15, eta = 0.01, random_state = 1)\n",
        "\n",
        "# # # Train the model\n",
        "# # model2.fit(X_std, y)\n",
        "\n",
        "# # # Plot the training errors of both of the models\n",
        "# # plt.plot(range(1, len(model2.cost_) + 1), model2.cost_, marker = 'x', color = 'blue')\n",
        "# # plt.xlabel('Epochs')\n",
        "# # plt.ylabel('Sum-squared-error')\n",
        "# # plt.show()\n",
        "\n",
        "\n",
        "# # # Plot the decision boundary\n",
        "# # pdr.plot_decision_regions(X_std, y,x0,x1, classifier = model2)\n",
        "# # plt.title('Adaline - Stochastic Gradient Descent')\n",
        "# # plt.xlabel('sepal length [standardized')\n",
        "# # plt.ylabel('petal length [standardized]')\n",
        "# # plt.legend(loc = 'upper left')\n",
        "# # plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctIhfkxOM2M"
      },
      "source": [
        "# plot2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35zTkYx13Xzq"
      },
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import offsetbox\n",
        "from sklearn import (manifold, decomposition, ensemble)\n",
        "\n",
        "X = X\n",
        "class_label = y\n",
        "print(class_label.size)\n",
        "# X = X.reshape((X.size // 52, 52))\n",
        "y2 = range(class_label.size)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "n_neighbors = 50\n",
        "n_com = 2\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Scale and visualize the embedding vectors\n",
        "def plot_embedding(X, title=None):\n",
        "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
        "    X = (X - x_min) / (x_max - x_min)\n",
        "\n",
        "    plt.figure()\n",
        "    ax = plt.subplot(111)\n",
        "    for i in range(X.shape[0]):\n",
        "      plt.plot(X[i][0], X[i][1], ('bo' if class_label[i] == 0 else 'ro'))\n",
        "        # plt.text(X[i, 0], X[i, 1], str(class_label[i][0]),\n",
        "                #  color=('r' if class_label[i][0] == 0 else 'b'),\n",
        "                #  fontdict={'weight': 'bold', 'size': 9})\n",
        "\n",
        "    if hasattr(offsetbox, 'AnnotationBbox'):\n",
        "        # only print thumbnails with matplotlib > 1.0\n",
        "        shown_images = np.array([[1., 1.]])  # just something big\n",
        "        for i in range(n_samples):\n",
        "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
        "            if np.min(dist) < 4e-3:\n",
        "                # don't show points that are too close\n",
        "                continue\n",
        "            shown_images = np.r_[shown_images, [X[i]]]\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Random 2D projection using a random unitary matrix\n",
        "print(\"Computing PCA projection\")\n",
        "t0 = time()\n",
        "ppca = decomposition.PCA(n_components=n_com)\n",
        "X_pca = ppca.fit_transform(X)\n",
        "plot_embedding(X_pca, \"Principal Components projection of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n",
        "\n",
        "# # ----------------------------------------------------------------------\n",
        "# # Projection on to the first 2 principal components\n",
        "\n",
        "# print(\"Computing Kernel PCA projection\")\n",
        "# t0 = time()\n",
        "# kpca = decomposition.KernelPCA(n_components=n_com)\n",
        "# X_kpca = kpca.fit_transform(X)\n",
        "# plot_embedding(X_kpca,\n",
        "#                \"Kernel Principal Components projection of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "# # ----------------------------------------------------------------------\n",
        "# # Projection on to the first 2 principal components\n",
        "\n",
        "# print(\"Computing Sparce PCA projection\")\n",
        "# t0 = time()\n",
        "# spca = decomposition.SparsePCA(n_components=n_com)\n",
        "# X_spca = spca.fit_transform(X)\n",
        "# plot_embedding(X_pca,\n",
        "#                \"Sparce Principal Components projection of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Isomap projection of the digits dataset\n",
        "print(\"Computing Isomap embedding\")\n",
        "t0 = time()\n",
        "iso = manifold.Isomap(n_neighbors, n_components=n_com)\n",
        "X_iso = iso.fit_transform(X)\n",
        "print(\"Done.\")\n",
        "plot_embedding(X_iso,\n",
        "               \"Isomap projection of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n",
        "\n",
        "# # ----------------------------------------------------------------------\n",
        "# # Locally linear embedding of the digits dataset\n",
        "# print(\"Computing LLE embedding\")\n",
        "# lle = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_com, method='standard')\n",
        "# t0 = time()\n",
        "# X_lle = lle.fit_transform(X)\n",
        "# print(\"Done. Reconstruction error: %g\" % lle.reconstruction_error_)\n",
        "# plot_embedding(X_lle,\n",
        "#                \"Locally Linear Embedding of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "\n",
        "# # ----------------------------------------------------------------------\n",
        "# # Modified Locally linear embedding of the digits dataset\n",
        "# print(\"Computing modified LLE embedding\")\n",
        "# mlle = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_com, method='modified')\n",
        "# t0 = time()\n",
        "# X_mlle = mlle.fit_transform(X)\n",
        "# print(\"Done. Reconstruction error: %g\" % mlle.reconstruction_error_)\n",
        "# plot_embedding(X_mlle,\n",
        "#                \"Modified Locally Linear Embedding of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "\n",
        "\n",
        "# # ----------------------------------------------------------------------\n",
        "# # LTSA embedding of the digits dataset\n",
        "# print(\"Computing LTSA embedding\")\n",
        "# ltsa = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_com, method='ltsa')\n",
        "# t0 = time()\n",
        "# X_ltsa = ltsa.fit_transform(X)\n",
        "# print(\"Done. Reconstruction error: %g\" % ltsa.reconstruction_error_)\n",
        "# plot_embedding(X_ltsa,\n",
        "#                \"Local Tangent Space Alignment of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "\n",
        "# # ----------------------------------------------------------------------\n",
        "# # MDS  embedding of the digits dataset\n",
        "# print(\"Computing MDS embedding\")\n",
        "# mds = manifold.MDS(n_components=n_com, n_init=1, max_iter=100)\n",
        "# t0 = time()\n",
        "# X_mds = mds.fit_transform(X)\n",
        "# print(\"Done. Stress: %f\" % mds.stress_)\n",
        "# plot_embedding(X_mds,\n",
        "#                \"MDS embedding of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "\n",
        "# # ----------------------------------------------------------------------\n",
        "# # Random Trees embedding of the digits dataset\n",
        "# print(\"Computing Totally Random Trees embedding\")\n",
        "# hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0, max_depth=5)\n",
        "# t0 = time()\n",
        "# X_transformed = hasher.fit_transform(X)\n",
        "# reduced = decomposition.TruncatedSVD(n_components=n_com)\n",
        "# X_reduced = reduced.fit_transform(X_transformed)\n",
        "\n",
        "# plot_embedding(X_reduced,\n",
        "#                \"Random forest embedding of the digits (time %.2fs)\" %\n",
        "#                (time() - t0))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Spectral embedding of the digits dataset\n",
        "print(\"Computing Spectral embedding\")\n",
        "se = manifold.SpectralEmbedding(n_components=n_com, random_state=0,\n",
        "                                eigen_solver=\"arpack\")\n",
        "t0 = time()\n",
        "X_se = se.fit_transform(X)\n",
        "\n",
        "plot_embedding(X_se,\n",
        "               \"Spectral embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# t-SNE embedding of the digits dataset\n",
        "print(\"Computing t-SNE embedding\")\n",
        "tsne = manifold.TSNE(n_components=n_com, init='pca', random_state=0)\n",
        "t0 = time()\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "plot_embedding(X_tsne,\n",
        "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsoO4TAjOUSP"
      },
      "source": [
        "# plot3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCKaRu8QswJh"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "h = .02  # step size in the mesh\n",
        "\n",
        "names = [\"Decision Tree\", \"Random Forest\", \"Neural Net\",\n",
        "         \"Naive Bayes\",\"MLPClassifier\",\"Adaline\",\"LinearRegression\"]\n",
        "\n",
        "classifiers = [\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    GaussianNB(),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    Adaline(epochs=30,eta=0.01,minibatches=None,random_seed=1),\n",
        "    LinearRegression()]\n",
        "\n",
        "X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
        "                           random_state=1, n_clusters_per_class=1)\n",
        "\n",
        "\n",
        "# X_train.reshape(1,-1)\n",
        "# y_train.reshape(1,-1)\n",
        "# X_test.reshape(1,-1)\n",
        "# model2=model1.fit(X_train,y_train)\n",
        "# yy=model2.predict(X_test)\n",
        "# rng = np.random.RandomState(2)\n",
        "# X += 2 * rng.uniform(size=X.shape)\n",
        "# linearly_separable = (X, y)\n",
        "\n",
        "# all_manifolds = [\n",
        "#     # (X_spca, 'X_spca', spca),\n",
        "#     (X_pca, 'X_pca', ppca),\n",
        "#     (X_iso, 'X_iso', iso),\n",
        "#     (X_lle, 'X_lle', lle),\n",
        "#     (X_mlle, 'X_mlle', mlle),\n",
        "#     (X_kpca, 'X_kpca', kpca),\n",
        "#     (X_ltsa, 'X_ltsa', ltsa),\n",
        "#     (X_mds, 'X_mds', mds),\n",
        "#     (X_reduced, 'X_reduced', reduced),\n",
        "#     (X_se, 'X_se', se),\n",
        "#     (X_tsne, '5', se)\n",
        "# ]\n",
        "\n",
        "datasets = [(X_pca, class_label), \n",
        "            (X_iso, class_label), \n",
        "            # (X_se, class_label),\n",
        "            (X_tsne, class_label)]\n",
        "\n",
        "figure = plt.figure(figsize=(27, 9))\n",
        "ii = 1\n",
        "# iterate over datasets\n",
        "for ds_cnt, ds in enumerate(datasets):\n",
        "    # preprocess dataset, split into training and test part\n",
        "    X, y = ds\n",
        "    y = [i[0] for i in y]\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=.4, random_state=42)\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n",
        "\n",
        "    # just plot the dataset first\n",
        "    cm = plt.cm.RdBu\n",
        "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "    # print \"len(datasets)\",len(datasets)\n",
        "    # print \"len(classifiers) + 1\",len(classifiers) + 1\n",
        "    ax = plt.subplot(len(datasets), len(classifiers) + 1, ii)\n",
        "    if ds_cnt == 0:\n",
        "        ax.set_title(\"Input data\")\n",
        "    # Plot the training points\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
        "               edgecolors='k')\n",
        "    # Plot the testing points\n",
        "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
        "               edgecolors='k')\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ii += 1\n",
        "\n",
        "    # iterate over classifiers\n",
        "    for name, clf in zip(names, classifiers):\n",
        "        ax = plt.subplot(len(datasets), len(classifiers) + 1,ii)\n",
        "        if name==\"Adaline\":\n",
        "          \n",
        "          # clf= MultiOutputClassifier(clf, n_jobs=-1)\n",
        "          yyyy=np.array(y_train,dtype=int)\n",
        "          # y_t=np.zeros((yyyy.size,1))\n",
        "          # y_t[:,0]=yyyy\n",
        "          # y_tt=np.array(y_t,dtype=int)\n",
        "          yyyy1=np.array(y_test,dtype=int)\n",
        "          # y_t1=np.zeros((yyyy1.size,1))\n",
        "          # y_t1[:,0]=yyyy1\n",
        "          # y_tt1=np.array(y_t1,dtype=int)\n",
        "          # y_t[:,1]=yyyy\n",
        "          clf.fit(X_train, yyyy)\n",
        "          score = clf.score(X_test, yyyy1)\n",
        "          Z=clf.predict(np.array([xx.ravel(), xx.ravel()]).T)\n",
        "          # print(multi_target_forest.predict_proba(X))\n",
        "        elif name==\"LinearRegression\":\n",
        "\n",
        "          clf.fit(X_train, y_train)\n",
        "          score = clf.score(X_test, y_test)\n",
        "          Z=clf.predict(np.array([xx.ravel(), xx.ravel()]).T)\n",
        "\n",
        "        else:\n",
        "          clf.fit(X_train, y_train)\n",
        "          score = clf.score(X_test, y_test)\n",
        "        \n",
        "\n",
        "        # Plot the decision boundary. For that, we will assign a color to each\n",
        "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "        elif hasattr(clf, \"predict_proba\") :\n",
        "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "\n",
        "        # Put the result into a color plot\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "\n",
        "        # Plot the training points\n",
        "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
        "                   edgecolors='k')\n",
        "        # Plot the testing points\n",
        "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
        "                   edgecolors='k', alpha=0.6)\n",
        "\n",
        "        ax.set_xlim(xx.min(), xx.max())\n",
        "        ax.set_ylim(yy.min(), yy.max())\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "        if ds_cnt == 0:\n",
        "            ax.set_title(name)\n",
        "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
        "                size=15, horizontalalignment='right')\n",
        "        ii += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}